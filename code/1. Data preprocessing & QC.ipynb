{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/Users/david/Dropbox/PhD/Scripts/Spatial analyses')\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geoplot\n",
    "import geoplot.crs as gcrs\n",
    "import datashader as ds, colorcet as cc\n",
    "# import holoviews as hv\n",
    "# hv.extension(\"bokeh\")\n",
    "# from holoviews.element.tiles import EsriImagery\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "import fiona\n",
    "import seaborn as sns\n",
    "from datashader.utils import export_image\n",
    "# from holoviews.operation.datashader import datashade\n",
    "import matplotlib.pyplot as plt\n",
    "import libpysal as lps\n",
    "from scipy.spatial import cKDTree\n",
    "from libpysal.weights.distance import get_points_array\n",
    "from esda import fdr\n",
    "import contextily as ctx\n",
    "import pyspace\n",
    "from importlib import reload\n",
    "plt.rc('font', family='Helvetica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classy_map_ch(df, col, cmap, title, _reversed = True):\n",
    "    filename = title+'.png'\n",
    "    if not os.path.isfile(result_folder/'Maps features'/filename):\n",
    "        if _reversed:\n",
    "            cmap += '_r'\n",
    "        ax = df.to_crs(21781).plot(col, markersize=0.05, linewidth = 0.1, cmap = cmap, legend = True,figsize = (8, 8), legend_kwds = {'shrink':0.5})\n",
    "        lakes.plot(color = 'lightblue', ax=ax)\n",
    "        cantons_ch.geometry.boundary.plot(ax=ax,edgecolor='k', color=None, linewidth=0.1)\n",
    "        plt.imshow(out_image.squeeze(),extent=ch_extent, cmap='Greys_r', alpha=0.4)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(title, fontsize=15, color= 'grey')\n",
    "        plt.savefig(result_folder/'Maps features'/filename, dpi=600, bbox_inches='tight')\n",
    "        return ax\n",
    "    else:\n",
    "        print('Map already generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(axs, orient=\"v\",digits = 2, fontsize = 8, space=.05):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.02)\n",
    "                value = '{:.{}f}'.format(p.get_height(), digits)\n",
    "                ax.text(_x, _y, value,size = fontsize, ha=\"center\") \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.{}f}'.format(p.get_width(), digits)\n",
    "                ax.text(_x, _y, value,size = fontsize, ha=\"left\")\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax, digits)\n",
    "    else:\n",
    "        _single(axs)\n",
    "def optimize_df(df):\n",
    "    \"\"\"\n",
    "    Convert each column of a pandas DataFrame to the datatype that takes the lowest memory.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The converted DataFrame with lowest memory datatypes for each column.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, convert all object columns to category type\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[obj_cols] = df[obj_cols].astype('category')\n",
    "\n",
    "    # Next, loop through all numeric columns and downcast the data types\n",
    "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            # Use smallest integer type possible\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "#         else:\n",
    "#             # Use smallest float type possible ! Bug 'halffloat' not supported by Arrow ! -> Commenting out\n",
    "#             c_min = df[col].min()\n",
    "#             c_max = df[col].max()\n",
    "#             if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "#                 df[col] = df[col].astype('float32')\n",
    "#             else:\n",
    "#                 df[col] = df[col].astype('float64')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder  = Path('../Data/')\n",
    "result_folder = Path('../Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insurance claims\n",
    "\n",
    "# Linkage\n",
    "df_paires_lamal_lca = pd.read_csv(data_folder/'max_probs_w_zipcode_pour_david_w_uuid.csv')\n",
    "\n",
    "dict_lamal_to_uuid = df_paires_lamal_lca.set_index('id_lamal')['uuid'].to_dict()\n",
    "dict_lca_to_uuid = df_paires_lamal_lca.set_index('id_lca')['uuid'].to_dict()\n",
    "\n",
    "dict_lamal_to_lca = df_paires_lamal_lca.set_index('id_lamal')['id_lca'].to_dict()\n",
    "dict_lca_to_lamal = df_paires_lamal_lca.set_index('id_lca')['id_lamal'].to_dict()\n",
    "\n",
    "df_aos_address = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_aos_address.parquet.gzip'))\n",
    "df_multiple_address_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_aos.parquet.gzip'))\n",
    "df_multiple_address_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_lca.parquet.gzip'))\n",
    "df_remaining_multiple_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_remaining_multiple_lca.parquet.gzip'))\n",
    "##\n",
    "df_couverture_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_couverture_aos.parquet.gzip'))\n",
    "df_flag_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_flag_aos.parquet.gzip'))\n",
    "df_drug_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_drug_aos.parquet.gzip'))\n",
    "df_couverture_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_couverture_lca.parquet.gzip'))\n",
    "df_prestation_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_prestation_aos.parquet.gzip'))\n",
    "df_prestation_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_prestation_lca.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prestation_aos_cam = pd.read_csv(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'Santeintegra_TARMED_080523.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tarmed_cam = {'0.1710':'Acupuncture',\n",
    "                   '0.1720':'Acupuncture',\n",
    "                   '0.1730':'Acupuncture',\n",
    "                   '0.1735':'Acupuncture',\n",
    "                   '0.1740':'Neural therapy',\n",
    "                   '0.1750':'Neural therapy',\n",
    "                   '0.1760':'Neural therapy',\n",
    "                   '0.1770':'Homeopathy',\n",
    "                   '0.1780':'Homeopathy',\n",
    "                   '0.1790':'Homeopathy',\n",
    "                   '0.1800':'Homeopathy',\n",
    "                   '0.1800':'Homeopathy',\n",
    "                   '0.1810':'Traditional Chinese medicine',\n",
    "                   '0.1820':'Traditional Chinese medicine',\n",
    "                   '0.1830':'Traditional Chinese medicine',\n",
    "                   '0.1840':'Anthroposophic medicine',\n",
    "                   '0.1850':'Anthroposophic medicine',\n",
    "                   '0.1860':'Anthroposophic medicine',\n",
    "                   '0.1870':'Phytotherapy',\n",
    "                   '0.1871':'Phytotherapy',\n",
    "                   '0.1872':'Phytotherapy',\n",
    "                   '0.1880':'Phone consultation',\n",
    "                   '0.1890':'Phone consultation',\n",
    "                   '0.1895':'Phone consultation',\n",
    "                   '0.1896':'Phone consultation',\n",
    "                   '0.1900':'Phone consultation',\n",
    "\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prestation_aos_cam['CDPOSITION_categories'] = df_prestation_aos_cam['CDPOSITION'].map(df_prestation_aos_cam['CDPOSITION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prestation_lca['PRESTATIONS_BRUTES'].sum()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a UUID so that we have a single unique ID instead of pairs of ID_LAMAL-ID_LCA\n",
    "df_prestation_lca['uuid'] = df_prestation_lca['ID_LCA'].map(dict_lca_to_uuid)\n",
    "df_prestation_aos['uuid'] = df_prestation_aos['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "df_prestation_aos_cam['uuid'] = df_prestation_aos_cam['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "\n",
    "df_couverture_lca['uuid'] = df_couverture_lca['ID_LCA'].map(dict_lca_to_uuid)\n",
    "df_couverture_aos['uuid'] = df_couverture_aos['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "\n",
    "df_drug_aos['uuid'] = df_drug_aos['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "df_flag_aos['uuid'] = df_flag_aos['ID_LAMAL'].map(dict_lamal_to_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GeoDataFrames\n",
    "df_aos_address = gpd.GeoDataFrame(df_aos_address, crs = 4326, geometry = gpd.points_from_xy(df_aos_address['lon_masked'], df_aos_address['lat_masked']))\n",
    "df_multiple_address_aos = gpd.GeoDataFrame(df_multiple_address_aos, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_aos['lon_masked'], df_multiple_address_aos['lat_masked']))\n",
    "df_multiple_address_lca = gpd.GeoDataFrame(df_multiple_address_lca, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_lca['lon_masked'], df_multiple_address_lca['lat_masked']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date strings to datetime objects\n",
    "df_multiple_address_aos['date'] = pd.to_datetime(df_multiple_address_aos['MIN_of_Date_adress'], format='%d%b%y')\n",
    "# Extract the month and year as separate columns\n",
    "df_multiple_address_aos['month'] = df_multiple_address_aos['date'].dt.month\n",
    "df_multiple_address_aos['year'] = df_multiple_address_aos['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date strings to datetime objects\n",
    "df_multiple_address_lca['date'] = pd.to_datetime(df_multiple_address_lca['MIN_of_Date_adress'], format='%d%b%y')\n",
    "# Extract the month and year as separate columns\n",
    "df_multiple_address_lca['month'] = df_multiple_address_lca['date'].dt.month\n",
    "df_multiple_address_lca['year'] = df_multiple_address_lca['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(df, year_col='ANNEE_TRAITEMENT', month_col='MOIS_TRAITEMENT'):\n",
    "    df['treatmentdate'] = pd.to_datetime(df[year_col].astype('string') + '-' + df[month_col].astype('string'))\n",
    "    df['treatmentmonth'] = df['treatmentdate'].dt.strftime('%Y-%m')\n",
    "    df['treatment_Q'] = df['treatmentdate'].dt.to_period(\"Q\")\n",
    "    return df\n",
    "\n",
    "df_list = [df_prestation_aos, df_prestation_lca, df_prestation_aos_cam, df_drug_aos]\n",
    "for df in df_list:\n",
    "    df = process_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dfs\n",
    "df_prestation_aos = optimize_df(df_prestation_aos)\n",
    "df_prestation_lca = optimize_df(df_prestation_lca)\n",
    "df_prestation_aos_cam = optimize_df(df_prestation_aos_cam)\n",
    "df_drug_aos = optimize_df(df_drug_aos)\n",
    "df_couverture_aos = optimize_df(df_couverture_aos)\n",
    "df_couverture_lca = optimize_df(df_couverture_lca)\n",
    "df_flag_aos = optimize_df(df_flag_aos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ID not present in the respective datasets\n",
    "df_prestation_aos['ID_LCA'] = df_prestation_aos['ID_LAMAL'].map(dict_lamal_to_lca)\n",
    "df_prestation_lca['ID_LAMAL'] = df_prestation_lca['ID_LCA'].map(dict_lca_to_lamal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cantons = {'AG':'Aargau',\n",
    "                'AI':'Appenzell Innerrhoden',\n",
    "                'AR':\"Appenzell Ausserrhoden\",\n",
    "                'BE':'Bern',\n",
    "                'BL':'Basel-Landschaft',\n",
    "                'BS':'Basel-Stadt',\n",
    "                'FR':'Fribourg',\n",
    "                'GE':'Genève',\n",
    "                'GL':'Glarus',\n",
    "                'GR':'Graubünden',\n",
    "                'JU':\"Jura\",\n",
    "                'LU':'Luzern',\n",
    "                'NE':'Neuchâtel',\n",
    "                'NW':'Nidwalden',\n",
    "                'OW':'Obwalden',\n",
    "                'SH':'Schaffhausen',\n",
    "                'SZ':'Schwyz',\n",
    "                'SO':'Solothurn',\n",
    "                'SG':'St. Gallen',\n",
    "                'TG':'Thurgau',\n",
    "                'TI':'Ticino',\n",
    "                'UR':'Uri',\n",
    "                'VS':'Valais',\n",
    "                'VD':'Vaud',\n",
    "                'ZG':'Zug',\n",
    "                'ZH':'Zürich'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_couverture_aos['CANTON_NAME'] = df_couverture_aos['CDCANTON_POST'].map(dict_cantons)\n",
    "df_couverture_lca['CANTON_NAME'] = df_couverture_lca['CDCANTON_POST'].map(dict_cantons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prestation_aos.to_parquet('../Data/processed/df_prestation_aos_preprocessed.parquet.gzip', compression='gzip')\n",
    "df_prestation_aos_cam.to_parquet('../Data/processed/df_prestation_aos_cam_preprocessed.parquet.gzip', compression='gzip')\n",
    "df_prestation_lca.to_parquet('../Data/processed/df_prestation_lca_preprocessed.parquet.gzip', compression='gzip')\n",
    "df_drug_aos.to_parquet('../Data/processed/df_drug_aos_preprocessed.parquet.gzip', compression='gzip')\n",
    "df_couverture_aos.to_parquet('../Data/processed/df_couverture_aos_preprocessed.parquet.gzip', compression='gzip')\n",
    "df_couverture_lca.to_parquet('../Data/processed/df_couverture_lca_preprocessed.parquet.gzip', compression='gzip')\n",
    "df_flag_aos.to_parquet('../Data/processed/df_flag_aos_preprocessed.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Solve multiple addresses and duplicates situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_couverture_aos.ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_lca[df_multiple_address_lca.ID_LCA == 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos[df_multiple_address_aos.ID_LAMAL.isin(df_aos_address.id_lamal)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_lca[df_multiple_address_lca.ID_LCA.isin(df_aos_address.id_lca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos[df_multiple_address_aos.ID_LAMAL.isin(df_aos_address.id_lamal)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address[df_aos_address.id_lamal == 103327].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Calculate distance between each duplicate to evaluate if some are far from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_lca = df_aos_address[df_aos_address.id_lca.duplicated()].id_lca.unique()\n",
    "df_duplicated = df_aos_address[df_aos_address.id_lca.isin(duplicated_lca)]\n",
    "grouped_dupli = df_duplicated.groupby('id_lca', observed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_lamal = df_aos_address[df_aos_address.id_lamal.duplicated()].id_lamal.unique()\n",
    "duplicated_lamal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address.drop_duplicates(['id_lamal','id_lca','address_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[df_duplicated.id_lca == 863968]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated.crs = 4326\n",
    "df_duplicated = df_duplicated.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[\"distance\"] = df_duplicated.groupby(\"id_lca\", observed = True)[\"geometry\"].apply(lambda x: x.distance(x.shift()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[~df_duplicated['distance'].isnull()].sort_values('distance')['distance'].plot.hist(bins = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[df_duplicated.id_lamal == 1478147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[~df_duplicated['distance'].isnull()].sort_values('distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[df_duplicated.id_lamal ==1043747]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Suppression des doublons d'adresses multiples\n",
    "\n",
    "This code is working with a dataframe called df_multiple_address_aos, and it performs a number of operations on the dataframe in a specific order:\n",
    "\n",
    "`sort_values(['date','address_id'])`: This sorts the dataframe by the values in the 'date' column, and then by the values in the 'address_id' column. This means that the rows will be arranged in the dataframe based on the date, and for any rows that have the same date, they will be further sorted by the value in the 'address_id' column.\n",
    "\n",
    "`drop_duplicates(subset = ['ID_LAMAL','address_id'])`: This method removes any duplicates in the dataframe based on the specified subset of columns. In this case, it checks for duplicate rows based on the values in both 'ID_LAMAL' and 'address_id' columns, it keeps only the first occurrence of each set of duplicates and remove the others, this will keep unique combination of those two columns in the dataframe.\n",
    "\n",
    "`drop_duplicates(['ID_LAMAL','date'])`: Similarly, this method also checks for duplicates in the dataframe, but this time it checks for duplicate rows based on the values in both 'ID_LAMAL' and 'date' columns and remove the duplicates.\n",
    "\n",
    "At the end of these three operations, the new dataframe that is being assigned to df_multiple_address_aos_no_dupli should have all the rows sorted by date and address_id, and with no duplicate values based on 'ID_LAMAL' & 'address_id' combination and 'ID_LAMAL' & 'date' combination\n",
    "\n",
    "It's also worth to mention that if there is any missing value in either 'ID_LAMAL', 'address_id' or 'date' columns, it may result in an error, or unexpected result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos[df_multiple_address_aos.ID_LAMAL ==147408].sort_values(['date','address_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos_no_dupli = df_multiple_address_aos.sort_values(['date','address_id']).drop_duplicates(subset = ['ID_LAMAL','address_id']).drop_duplicates(['ID_LAMAL','date'])\n",
    "df_multiple_address_lca_no_dupli = df_multiple_address_lca.sort_values(['date','address_id']).drop_duplicates(subset = ['ID_LCA','address_id']).drop_duplicates(['ID_LCA','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos_no_dupli[df_multiple_address_aos_no_dupli.ID_LAMAL == 256935]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos_no_dupli['ID_LCA'] = df_multiple_address_aos_no_dupli['ID_LAMAL'].map(df_paires_lamal_lca.set_index('id_lamal')['id_lca'].to_dict())\n",
    "df_multiple_address_lca_no_dupli['ID_LAMAL'] = df_multiple_address_lca_no_dupli['ID_LCA'].map(df_paires_lamal_lca.set_index('id_lca')['id_lamal'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Suppression des doublons de df_aos_address\n",
    "\n",
    "Premièrement, j'ai enlevé un doublon complet (càd une paire ID_LAMAL-ID_LCA ayant le même address_id). A noter que bizarrement, ce même address_id possède des coord. géo. différentes ! Il s'agit de l'ID_LAMAL = 1477326 et de l'address_id = 147133.\n",
    "\n",
    "Ensuite, j'ai noté qu'il y avait **19,111** ID_LAMAL dupliqués parmi le dataset `AOS_address_masked_ssdoubl` alors que...vu son nom, j'imagine que celui-ci ne devrait pas en contenir. Du fait des duplications, ces 19,111 ID_LAMAL sont associés à **39,804** `ID_LAMAL-ID_LCA-address_id`.\n",
    "\n",
    "Après quelques investigations, et vu qu'ici je n'ai pas de colonne `MIN_of_Date_address` je ne suis pas en mesure de définir quel `address_id` est à conserver lorsqu'il y en a plusieurs pour une même paire ID_LAMA-ID_LCA.\n",
    "\n",
    "J'ai calculé la distance entre les différentes address_id qui existent pour une même paire et on obtient des distances qui sont pour la plupart assez élevée et même parfois à plus de 200km (par ex. `ID_LAMAL == 1043747`)! \n",
    "\n",
    "Pour l'instant, je vais avancer sur les analyses avec l'approche la plus basique : ordonner les paires par address_id croissant et converser le premier address_id (celui ayant la valeur la plus faible donc). Les 19,111 paires conservées sont ajoutées  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicated[df_duplicated['distance'] > 1000].sort_values('distance').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address[df_aos_address.id_lamal == 1477326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep first occurrence for duplicated pairs\n",
    "df_aos_address_nodupli = df_aos_address.sort_values(['id_lamal','id_lca','address_id']).drop_duplicates(subset = ['id_lamal','id_lca'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address_nodupli['year'] = 2017\n",
    "df_aos_address_nodupli['month'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address_nodupli = df_aos_address_nodupli.rename(columns = {'id_lamal':'ID_LAMAL','id_lca':'ID_LCA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_address_full = pd.concat([df_aos_address_nodupli[['ID_LAMAL','ID_LCA','year','month','address_id','lon_masked','lat_masked','geometry']],\n",
    "                                 df_multiple_address_aos_no_dupli[['ID_LAMAL','ID_LCA','year','month','address_id','lon_masked','lat_masked','geometry']].drop_duplicates(), \n",
    "                                 df_multiple_address_lca_no_dupli[['ID_LAMAL','ID_LCA','year','month','address_id','lon_masked','lat_masked','geometry']].drop_duplicates()]).drop_duplicates(subset = ['ID_LAMAL','ID_LCA','year','month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_lca_no_dupli[['ID_LAMAL','ID_LCA','year','month','address_id','lon_masked','lat_masked','geometry']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Update : New address data sent by Christophe on January 27th, 2023\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTES</b> \n",
    "Voici les adresses. Oui, il y a un seul fichier ZIP.\n",
    "    \n",
    "Tu connais déjà les 3 fichiers adresses. Ils devraient être tip-top (AOS_masked_ssdoubl.csv  Output_adress_doubl_AOS.csv Output_adress_doubl_LCA.csv).\n",
    "    \n",
    "Tu devrais déjà avoir quasiment tout ce que tu désires dans ces 3 fichiers.\n",
    " \n",
    "Toutefois, certaines jointures n’ont rien donné, quand j’ai essayé de retrouver l’adresse ID depuis mes propres adresses par exemple en liant à ton fichier GEOCODED (cf requetes dans annexe ci-dessous).\n",
    "En conséquence j’avais pour certaines ID (assez rares) des missing (‘mis’ dans les noms de fichier qui suit).\n",
    "Pour ces missing, j’ai gardé le min de la date si je l’avais, min_LCA ou _LAM (c’est écrit ainsi dans le fichier). Si le min n’est pas donné c’est que je ne l’ai pas.\n",
    "    \n",
    "Puis j’ai cherché pour chaque assuré en même temps dans le fichier LAMAL des adresses (ca a donné le fichier LAMAL_miss_masked.csv) et après dans le fichier LCA des adresses du GM (LCA_miss_masked.csv).\n",
    "Ainsi ces deux fichiers sont plus ou moins mal-fichus et auront souvent les mêmes information (car l’assuré est en même temps Lama et LCA), mais je te les envoie car tu pourras encore en tirer quelques adresses supplémentaires.\n",
    "\n",
    "</div> \n",
    "\n",
    "- This update should solve the duplication problems.\n",
    "- There is a question mark on the `miss` df that he sent, not sure what these contain and how their info can be integrated to the rest of the data.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_aos_address_updated.parquet.gzip'))\n",
    "df_multiple_address_aos_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_aos_updated.parquet.gzip'))\n",
    "df_multiple_address_lca_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_lca_updated.parquet.gzip'))\n",
    "df_remaining_multiple_lca_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_remaining_multiple_lca_updated.parquet.gzip'))\n",
    "df_remaining_multiple_aos_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_remaining_multiple_aos_updated.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos_updated['date'] = pd.to_datetime(df_multiple_address_aos_updated['MIN_of_Date_adress'], format='%d%b%y')\n",
    "df_multiple_address_lca_updated['date'] = pd.to_datetime(df_multiple_address_lca_updated['MIN_of_Date_adress'], format='%d%b%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GeoDataFrames\n",
    "df_aos_address_updated = gpd.GeoDataFrame(df_aos_address_updated, crs = 4326, geometry = gpd.points_from_xy(df_aos_address_updated['lon_masked'], df_aos_address_updated['lat_masked'])).to_crs(2056)\n",
    "df_multiple_address_aos_updated = gpd.GeoDataFrame(df_multiple_address_aos_updated, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_aos_updated['lon_masked'], df_multiple_address_aos_updated['lat_masked'])).to_crs(2056)\n",
    "df_multiple_address_lca_updated = gpd.GeoDataFrame(df_multiple_address_lca_updated, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_lca_updated['lon_masked'], df_multiple_address_lca_updated['lat_masked'])).to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old version AOS had :', df_aos_address.shape[0], 'lines')\n",
    "print('Updated version AOS had :', df_aos_address_updated.shape[0], 'lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address.id_lamal.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old version AOS had :', df_multiple_address_aos.shape[0], 'lines')\n",
    "print('Updated version AOS had :', df_multiple_address_aos_updated.shape[0], 'lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old version AOS had :', df_multiple_address_aos.ID_LAMAL.nunique(), 'lines')\n",
    "print('Updated version AOS had :', df_multiple_address_aos_updated.ID_LAMAL.nunique(), 'lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old version AOS had :', df_multiple_address_lca.shape[0], 'lines')\n",
    "print('Updated version AOS had :', df_multiple_address_lca_updated.shape[0], 'lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Updated version AOS had :', df_remaining_multiple_aos_updated.shape[0], 'lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Updated version AOS had :', df_remaining_multiple_lca_updated.shape[0], 'lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Unique address AOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous code now deleted, we saw that there is one ID_LAMAL for which there is two address_id\n",
    "df_aos_address_updated[df_aos_address_updated.duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address_updated[df_aos_address_updated.duplicated(subset = ['ID_LAMAL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the duplicate, the one we choose is unimportant\n",
    "df_aos_address_updated = df_aos_address_updated.drop_duplicates(['ID_LAMAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### Multiple address AOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_multi_aos = df_multiple_address_aos_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply, if any row has the same ID_LAMAL, year, address_id, lon and lat...then it's just a duplicated row, not multiple addresses\n",
    "df_test_multi_aos_nouselessdupli = df_test_multi_aos.sort_values(['NOANNEE','MIN_of_Date_adress']).drop_duplicates(subset = ['ID_LAMAL','address_id','lon_masked','lat_masked'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_multi_aos_nouselessdupli.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check we still have the same number of ID_LAMAL\n",
    "df_test_multi_aos_nouselessdupli.ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "All good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ID_LAMAL having multiple address_id\n",
    "dupli_aos = df_test_multi_aos_nouselessdupli[df_test_multi_aos_nouselessdupli.ID_LAMAL.duplicated()].ID_LAMAL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, there are plenty of these that don't have multiple addresses (great data processing on GM's part...)\n",
    "# This should actually be together with df_aos_address\n",
    "df_test_multi_notmultipleaddress = df_test_multi_aos_nouselessdupli[df_test_multi_aos_nouselessdupli.ID_LAMAL.isin(dupli_aos) == False].sort_values(['ID_LAMAL','NOANNEE'])\n",
    "new_df_multiple_address_aos = pd.concat([df_aos_address_updated, df_test_multi_notmultipleaddress[['ID_LAMAL','address_id','lon_masked','lat_masked']]])\n",
    "new_df_multiple_address_aos['NOANNEE'] = 2017\n",
    "new_df_multiple_address_aos['MIN_of_Date_adress'] = '10JAN17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the ones that actually have real multiple addresses\n",
    "df_test_multi_aos_realmultiaddress = df_test_multi_aos_nouselessdupli[df_test_multi_aos_nouselessdupli.ID_LAMAL.isin(dupli_aos)].sort_values(['ID_LAMAL','NOANNEE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So...we actually have a lot of cases with two addresses...at the same date, which one is it???\n",
    "# We appear to have cases where the coordinates are almost exactly the same (0.00X differences), these can be identified by distance calculation and collasped into one\n",
    "same_date_different_address = df_test_multi_aos_realmultiaddress[df_test_multi_aos_realmultiaddress.duplicated(subset = ['ID_LAMAL','date'])].ID_LAMAL.unique()\n",
    "df_same_date_different_address = df_test_multi_aos_realmultiaddress[df_test_multi_aos_realmultiaddress.ID_LAMAL.isin(same_date_different_address)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we have the ones with multiples addresses at different dates aka no problem !\n",
    "df_test_multi_aos_realmultiaddress_noproblem = df_test_multi_aos_realmultiaddress[df_test_multi_aos_realmultiaddress.ID_LAMAL.isin(same_date_different_address) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating distance between addresses of each individual\n",
    "df_same_date_different_address[\"distance\"] = df_same_date_different_address.groupby(\"ID_LAMAL\", observed = True)[\"geometry\"].apply(lambda x: x.distance(x.shift()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the ones with addresses located at less than or equal to 500m, n = 8,097\n",
    "# For these, I could just pick one address per date, assuming that it won't change much since they are very close\n",
    "same_date_different_address_closedistance = df_same_date_different_address[df_same_date_different_address['distance'] <= 500].ID_LAMAL.unique()\n",
    "same_date_different_address_fardistance = df_same_date_different_address[df_same_date_different_address['distance'] > 500].ID_LAMAL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_same_date_different_address_closedistance  = df_same_date_different_address[df_same_date_different_address.ID_LAMAL.isin(same_date_different_address_closedistance)]\n",
    "df_same_date_different_address_fardistance  = df_same_date_different_address[df_same_date_different_address.ID_LAMAL.isin(same_date_different_address_fardistance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an example of different addresses being at the same date for one individual...very weird\n",
    "ax = df_same_date_different_address_fardistance.sort_values('ID_LAMAL').head(15).plot('ID_LAMAL', categorical = True, legend = True)\n",
    "ctx.add_basemap(ax, source=ctx.providers.Stamen.TonerLite,crs = 'EPSG:2056')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only check for a distance threshold some addresses may have one under 500 and another one above, if there is even one under, we don't want it in the far dataset, so we remove it\n",
    "df_same_date_different_address_fardistance = df_same_date_different_address_fardistance[df_same_date_different_address_fardistance.ID_LAMAL.isin(same_date_different_address_closedistance) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking one address per date for close distance duplicates\n",
    "df_same_date_different_address_closedistance_nodupli = df_same_date_different_address_closedistance.drop_duplicates(['MIN_of_Date_adress','ID_LAMAL'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking one address per date for far distance duplicates, we order by ID, date and distance (NaN being last), and keep the last\n",
    "df_same_date_different_address_fardistance_nodupli = df_same_date_different_address_fardistance.sort_values(['ID_LAMAL','date','distance']).drop_duplicates(['ID_LAMAL','date'], keep = 'last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### Putting AOS back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Real unique addresses\n",
    "print(df_aos_address_updated.ID_LAMAL.nunique())\n",
    "# 2 \"Fake\" multiple addresses\n",
    "print(df_test_multi_notmultipleaddress.ID_LAMAL.nunique())\n",
    "# 3 Non-problematic multiple addresses\n",
    "print(df_test_multi_aos_realmultiaddress_noproblem.ID_LAMAL.nunique())\n",
    "# 4 Problematic - Same date different addresses - Close distance duplicates\n",
    "print(df_same_date_different_address_closedistance_nodupli.ID_LAMAL.nunique())\n",
    "# 5 Problematic - Same date different addresses - Far distance duplicates\n",
    "print(df_same_date_different_address_fardistance_nodupli.ID_LAMAL.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the different datasets to create a final AOS address df\n",
    "df_aos_addresses_final = pd.concat([df_aos_address_updated,\n",
    "          df_test_multi_notmultipleaddress,\n",
    "          df_test_multi_aos_realmultiaddress_noproblem,\n",
    "          df_same_date_different_address_closedistance_nodupli,\n",
    "          df_same_date_different_address_fardistance_nodupli])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there is any unwanted duplicates\n",
    "print('Number of duplicates: ', df_aos_addresses_final[df_aos_addresses_final.duplicated(subset = ['ID_LAMAL','date'])].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the final number of ID_LAMAL\n",
    "print('Number of ID_LAMAL: ', df_aos_addresses_final.ID_LAMAL.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final['uuid'] = df_aos_addresses_final['ID_LAMAL'].map(dict_lamal_to_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining_multiple_aos_updated.ID_Lamal.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is this quite mysterious dataset sent by Christophe, with remaining multiple AOS\n",
    "# We can check if there is any ID_LAMAL there that isn't already in our final one\n",
    "df_remaining_multiple_aos_updated[df_remaining_multiple_aos_updated.ID_Lamal.isin(df_aos_addresses_final.ID_LAMAL) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "No...let's forget about this one and move forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## LCA addresses\n",
    "\n",
    "While Christophe sent a df `df_multiple_address_lca_updated`, we don't actually need it. It would even complicate things to use it since we would have to make sure that we keep the exact same address as for AOS when dropping duplicates. The simplest way of doing this is to simply make the link between AOS and LCA and use the addresses from the AOS dataset. In the end, this is the same individual...why would one have a different addresses for AOS and LCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final['ID_LCA'] = df_aos_addresses_final['ID_LAMAL'].map(df_paires_lamal_lca.set_index('id_lamal')['id_lca'].to_dict())\n",
    "# df_multiple_address_lca_no_dupli['ID_LAMAL'] = df_multiple_address_lca_no_dupli['ID_LCA'].map(df_paires_lamal_lca.set_index('id_lca')['id_lamal'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "## Geographical units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes = gpd.read_file(\"/Users/david/Dropbox/PhD/GitHub/COVID19/input/g2s15.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "### Country boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_geo = gpd.read_file(\"/Users/david/Dropbox/PhD/GitHub/COVID19/input/g2l15.shp\")\n",
    "with fiona.open(\"/Users/david/Dropbox/PhD/GitHub/COVID19/input/g2l15.shp\", \"r\") as shapefile:\n",
    "    country_geo_fiona = [feature[\"geometry\"] for feature in shapefile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_extent = np.asarray(country_geo.bounds)[0][[0,2,1,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file:\n",
    "relief_raster = rasterio.open('/Users/david/Dropbox/PhD/GitHub/COVID19/input/02-relief-ascii.asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_image, out_transform = rasterio.mask.mask(relief_raster, country_geo_fiona, crop=True, filled=False)\n",
    "out_meta = relief_raster.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "### Cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch = gpd.read_file('/Users/david/Dropbox/PhD/GitHub/SanteIntegra/Data/raw/Linkage/swissBOUNDARIES3D_1_3_TLM_KANTONSGEBIET.shp')\n",
    "cantons_ch = cantons_ch.to_crs(21781)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "## Demography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "statpop = pd.read_csv('/Users/david/Dropbox/PhD/Data/Databases/OFS/ag-b-00.03-vz2020statpop/STATPOP2020.csv',sep = ';')\n",
    "statpop_ha = statpop.copy()\n",
    "geometry = [Point(xy) for xy in zip(statpop['E_KOORD'], statpop['N_KOORD'])]\n",
    "statpop_point = gpd.GeoDataFrame(statpop, crs=2056, geometry=geometry)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(statpop_ha.E_KOORD, statpop_ha.N_KOORD)]\n",
    "statpop_ha = gpd.GeoDataFrame(statpop_ha, crs=2056, geometry=geometry)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "## Logement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "regbl_db = Path('/Users/david/Dropbox/PhD/GitHub/Swiss-Geocoder/Data/Raw_data/GWR_MADD_Export_MADD-20210920-A4_20220427/')\n",
    "regbl_df_A = optimize_df(pd.read_csv(regbl_db/'GWR_MADD_EIN-03_Data_MADD-20210920-A4_20220427.dsv', sep = '\\t'))\n",
    "regbl_df_B = optimize_df(pd.read_csv(regbl_db/'GWR_MADD_GEB-03_Data_MADD-20210920-A4_20220427.dsv', sep = '\\t'))\n",
    "regbl_df_C = optimize_df(pd.read_csv(regbl_db/'GWR_MADD_GST-03_Data_MADD-20210920-A4_20220427.dsv', sep = '\\t'))\n",
    "regbl_df_D = optimize_df(pd.read_csv(regbl_db/'GWR_MADD_WHG-03_Data_MADD-20210920-A4_20220427.dsv', sep = '\\t'))\n",
    "\n",
    "regbl_df_A_codebook = pd.read_csv(regbl_db/'GWR_MADD_EIN-03_Readme_MADD-20210920-A4_20220427.dsv', sep = '\\t')\n",
    "regbl_df_B_codebook = pd.read_csv(regbl_db/'GWR_MADD_GEB-03_Readme_MADD-20210920-A4_20220427.dsv', sep = '\\t')\n",
    "regbl_df_C_codebook = pd.read_csv(regbl_db/'GWR_MADD_GST-03_Readme_MADD-20210920-A4_20220427.dsv', sep = '\\t')\n",
    "regbl_df_D_codebook = pd.read_csv(regbl_db/'GWR_MADD_WHG-03_Readme_MADD-20210920-A4_20220427.dsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lst = pd.read_pickle('../Data/processed/gdf_lst.pkl')\n",
    "gdf_lst = gpd.GeoDataFrame(gdf_lst, geometry = gdf_lst['geometry'])\n",
    "gdf_ndvi = pd.read_pickle('../Data/processed/gdf_ndvi.pkl')\n",
    "gdf_ndvi = gpd.GeoDataFrame(gdf_ndvi, geometry = gdf_ndvi['geometry'])\n",
    "pm10 = pd.read_pickle('../Data/processed/pm10_2020.pkl')\n",
    "# pm10 = gpd.GeoDataFrame(pm10, geometry = pm10['geometry'])\n",
    "pm25 = pd.read_pickle('../Data/processed/pm25_2020.pkl')\n",
    "# pm25 = gpd.GeoDataFrame(pm25, geometry = pm25['geometry'])\n",
    "no2 = pd.read_pickle('../Data/processed/no2_2020.pkl')\n",
    "# no2_2020 = gpd.GeoDataFrame(no2_2020, geometry = no2_2020['geometry'])\n",
    "ns_car_day = pd.read_pickle('../Data/processed/ns_car_day.pkl')\n",
    "# ns_car_day = gpd.GeoDataFrame(ns_car_day, geometry = ns_car_day['geometry'])\n",
    "ns_car_night = pd.read_pickle('../Data/processed/ns_car_night.pkl')\n",
    "# ns_car_night = gpd.GeoDataFrame(ns_car_night, geometry = ns_car_night['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10 = pd.DataFrame(pm10).rename(columns = {'mean':'mean_pm10','median':'median_pm10'})\n",
    "pm25 = pd.DataFrame(pm25).rename(columns = {'mean':'mean_pm25','median':'median_pm25'})\n",
    "no2 = pd.DataFrame(no2).rename(columns = {'mean':'mean_no2','median':'median_no2'})\n",
    "ns_car_day = pd.DataFrame(ns_car_day).rename(columns = {'mean':'mean_carday','median':'median_carday'})\n",
    "ns_car_night = pd.DataFrame(ns_car_night).rename(columns = {'mean':'mean_carnight','median':'median_carnight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_df = pd.concat([pm10, pm25, no2, ns_car_day, ns_car_night], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_replacement(column):\n",
    "    conds = [column > np.percentile(column, 99.9)]\n",
    "    choices = [np.percentile(column, 50)]\n",
    "    return np.select(conds,choices,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_df = pollution_df.apply(lambda x: pct_replacement(x))\n",
    "pollution_df = pd.concat([statpop_ha[['RELI','geometry']], pollution_df], axis = 1)\n",
    "pollution_df = gpd.GeoDataFrame(pollution_df, crs = 2056, geometry = pollution_df['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting polygon geometries to point using centroids of polygons for NDVI\n",
    "gdf_ndvi['geometry_pt'] = gdf_ndvi['geometry'].centroid\n",
    "gdf_ndvi = gdf_ndvi.set_geometry(\"geometry_pt\")\n",
    "\n",
    "# Converting polygon geometries to point using centroids of polygons for LST\n",
    "gdf_lst['geometry_pt'] = gdf_lst['geometry'].centroid\n",
    "gdf_lst = gdf_lst.set_geometry(\"geometry_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(gdf_ndvi, 'mean_ndvi','RdYlGn', 'Indice de végétation', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(gdf_lst, 'mean_lst','magma', 'Température de surface (ºC)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting polygon geometries to point using centroids of polygons for pollution variables\n",
    "pollution_df['geometry_pt'] = pollution_df['geometry'].centroid\n",
    "pollution_df = pollution_df.set_geometry(\"geometry_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_pm10','magma', 'Pollution atmosphérique (PM10)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_pm25','magma', 'Pollution atmosphérique (PM25)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_no2','magma', 'Pollution atmosphérique (NO2)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_carday','magma', 'Pollution sonore (dB)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "## Socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_sep1 = optimize_df(gpd.read_file(data_folder/'raw'/'Swiss-SEP'/'SNC_Swiss-SEP1'/'SHP'/'ssep_user_geo.shp', driver = 'Shapefile'))\n",
    "df_swiss_sep2 = optimize_df(gpd.read_file(data_folder/'raw'/'Swiss-SEP'/'SNC_Swiss-SEP2'/'SHP'/'ssep2_user_geo.shp', driver = 'Shapefile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_2023 = optimize_df(pd.read_csv(data_folder/'raw'/'Swiss-SEP'/'SNC_Swiss-SEP-2023'/'ssep_open.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_2023 = gpd.GeoDataFrame(df_swiss_2023, crs = 2056, geometry=gpd.points_from_xy(df_swiss_2023.geox, df_swiss_2023.geoy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch = cantons_ch.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch[cantons_ch.NAME == 'Genève']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_sep_ge = df_swiss_2023[df_swiss_2023.within(cantons_ch.loc[20].geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_sep_ge[['ssep3','geometry']].explore('ssep3', cmap = 'RdYlGn', marker_kwds = {'radius':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_swiss_sep_ge.plot('ssep3', cmap = 'RdYlGn', markersize = 0.5, figsize = (15,15), linewidth=0, legend = True, legend_kwds={'shrink':0.5})\n",
    "ax.set_axis_off()\n",
    "plt.savefig(result_folder/'Maps features'/'Indice de statut socioéconomique - GE.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_swiss_sep_ge.plot('ssep3_d', cmap = 'RdYlGn', vmin = 0, vmax = 10, markersize = 0.5, figsize = (15,15), linewidth=0, legend = True, legend_kwds={'shrink':0.5})\n",
    "ax.set_axis_off()\n",
    "plt.savefig(result_folder/'Maps features'/'Indice de statut socioéconomique (Déciles) - GE.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_swiss_sep2, 'ssep2_d','RdYlGn', 'Indice de statut socioéconomique (Déciles)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_swiss_sep2, 'ssep2','RdYlGn', 'Indice de statut socioéconomique', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_swiss_2023, 'ssep3_d','RdYlGn', 'Indice de statut socioéconomique 2023 (Déciles)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## Accessibilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_access_ofs = pd.read_csv('/Users/david/Dropbox/PhD/GitHub/SanteIntegra/Data/raw/OFS/Accessibility/ag-b-00.03-2018spop-csv.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_access_ofs = gpd.GeoDataFrame(df_access_ofs, crs = 2056, geometry = gpd.points_from_xy(df_access_ofs.E_COORD, df_access_ofs.N_COORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_access_ofs, 'D_MEDIC', 'RdYlGn', 'Accès aux cabinets médicaux et centres ambulatoires')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_access_ofs, 'D_MEDIC_B', 'RdYlGn', 'Accès à la médecine de premier recours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_access_ofs, 'D_MEDIC_S', 'RdYlGn', 'Accès à la médecine spécialisée')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "### Add sociodemographic and environmental features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "Accessibility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(df_access_ofs.E_COORD, df_access_ofs.N_COORD)]\n",
    "\n",
    "df_access_ofs = df_access_ofs.set_geometry(geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access = gpd.sjoin_nearest(df_aos_addresses_final, df_access_ofs.drop(['RELI','E_COORD','N_COORD','YEAR','POP_TOTAL'], axis = 1), how = 'left', distance_col = 'distance_join_access').drop('index_right', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "Pollution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(pollution_df.geometry_pt.x, pollution_df.geometry_pt.y)]\n",
    "pollution_df = pollution_df.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution = gpd.sjoin_nearest(df_addresses_w_access, pollution_df.drop(['RELI','geometry_pt'], axis = 1), how = 'left', distance_col = 'distance_join_pollution').drop('index_right', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "NDVI & LST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ndvi_lst = pd.concat([gdf_ndvi.drop(['geometry_pt','geometry'], axis = 1), gdf_lst], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ndvi_lst = gdf_ndvi_lst.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst = gpd.sjoin_nearest(df_addresses_w_access_pollution, gdf_ndvi_lst.drop(['geometry_pt'], axis = 1), how = 'left', distance_col = 'distance_join_ndvi_lst').drop('index_right', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "Socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(df_swiss_2023.geox, df_swiss_2023.geoy)]\n",
    "df_swiss_2023 = df_swiss_2023.set_geometry(geometry)\n",
    "df_swiss_2023.crs = 2056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final_sep = gpd.sjoin_nearest(df_aos_addresses_final, df_swiss_2023, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final_sep = df_aos_addresses_final_sep.drop_duplicates(subset = ['ID_LAMAL','address_id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst_sep = pd.merge(df_addresses_w_access_pollution_ndvi_lst,df_aos_addresses_final_sep[['uuid','address_id','date','ssep2','ssep2_d','ssep2_t','ssep2_q','ssep3','ssep3_d','ssep3_t','ssep3_q']], on = ['uuid','address_id','date'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst_sep.to_parquet('../Data/processed/df_addresses_with_socio_env.parquet.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst_sep = gpd.GeoDataFrame(df_addresses_w_access_pollution_ndvi_lst_sep, crs = 4326, geometry=gpd.points_from_xy(df_addresses_w_access_pollution_ndvi_lst_sep.lon_masked, df_addresses_w_access_pollution_ndvi_lst_sep.lat_masked))\n",
    "df_addresses_w_access_pollution_ndvi_lst_sep_uniques = df_addresses_w_access_pollution_ndvi_lst_sep[df_addresses_w_access_pollution_ndvi_lst_sep.doubl.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of years\n",
    "years = [2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# repeat rows for each year\n",
    "df_addresses_w_access_pollution_ndvi_lst_sep_uniques = df_addresses_w_access_pollution_ndvi_lst_sep_uniques.reindex(df_addresses_w_access_pollution_ndvi_lst_sep_uniques.index.repeat(len(years)))\n",
    "df_addresses_w_access_pollution_ndvi_lst_sep_uniques['NOANNEE'] = years * (len(df_addresses_w_access_pollution_ndvi_lst_sep_uniques.index)//len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_address = pd.concat([df_addresses_w_access_pollution_ndvi_lst_sep_uniques, df_addresses_w_access_pollution_ndvi_lst_sep[df_addresses_w_access_pollution_ndvi_lst_sep.doubl.isnull() == False]]).reset_index(drop=True)\n",
    "df_full_address = df_full_address.sort_values(['uuid','date']).drop_duplicates(subset = ['uuid','NOANNEE'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_address.to_parquet('../Data/processed/df_full_address.parquet.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "## Distribution des assurés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = ds.Canvas(plot_width=1920, plot_height=1080).points(df_aos_addresses_final, 'lon_masked', 'lat_masked')\n",
    "export_image(ds.tf.set_background(ds.tf.shade(agg, cmap=cc.fire, how='eq_hist'), \"black\"),\"../Results/Maps features/assures_GM\")\n",
    "# export(tf.shade(agg, cmap = hot, how='eq_hist'),\"census_ds_mhot_eq_hist\")\n",
    "\n",
    "# img = ds.tf.shade(agg, cmap=cc.fire, how='eq_hist')\n",
    "# img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "## Représentation par canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_by_canton = df_couverture_aos.groupby('CANTON_NAME', observed = True).ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch['n_assures'] = cantons_ch['NAME'].map(n_by_canton.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch['EINWOHNERZ_filled'] = cantons_ch['NAME'].map(cantons_ch[~cantons_ch.EINWOHNERZ.isnull()].set_index('NAME')['EINWOHNERZ'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch['perc_assures'] = round((cantons_ch['n_assures'] / cantons_ch['EINWOHNERZ_filled']) *100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cantons_ch.to_crs(21781).plot('perc_assures', linewidth = 0.1, cmap = 'RdYlGn', legend = True, figsize = (8, 8), legend_kwds = {'shrink':0.5})\n",
    "lakes.plot(color = 'lightblue', ax=ax)\n",
    "plt.imshow(out_image.squeeze(),extent=ch_extent, cmap='Greys_r', alpha=0.4)\n",
    "ax.set_axis_off()\n",
    "cantons_ch[~cantons_ch.EINWOHNERZ.isnull()].apply(lambda x: ax.annotate(text=x.perc_assures, xy=x.geometry.centroid.coords[0], ha='center', size=12),\n",
    "               axis=1)\n",
    "ax.set_title(\"Pourcentage de la population totale représentée \\n dans les données du Groupe Mutuel\", fontsize=15, color= 'grey')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "# Couverture santé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/david/Dropbox/PhD/Scripts/Spatial analyses')\n",
    "import pyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_couverture_aos['date'] = pd.to_datetime(df_couverture_aos['NOMOIS'].astype('string') + '-' + df_couverture_aos['NOANNEE'].astype('string')).dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lamal_by_month = df_couverture_aos.groupby('date', observed = True).ID_LAMAL.nunique()\n",
    "n_lamal_by_year = df_couverture_aos.groupby('NOANNEE', observed = True).ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lamal_by_year = pd.DataFrame(n_lamal_by_year).reset_index()\n",
    "pal = sns.color_palette(\"Blues\", len(n_lamal_by_year))\n",
    "rank = n_lamal_by_year['ID_LAMAL'].argsort().argsort()   # http://stackoverflow.com/a/6266510/1628638\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "chart = sns.barplot(x=\"NOANNEE\", y=\"ID_LAMAL\", data=n_lamal_by_year, palette = np.array(pal)[rank], ax = ax)\n",
    "chart.set_xticklabels(chart.get_xticklabels(),size = 12, rotation=45, horizontalalignment='right')\n",
    "chart.set_xlabel(\"Année\",size = 14)\n",
    "chart.set_ylabel(\"Nombre d'assurés LAMAL\",size = 14)\n",
    "show_values(chart, digits = 0, fontsize = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {},
   "source": [
    "## Franchise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_franchise = df_couverture_aos[['ID_LAMAL','MTFRANCHISECOUV']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_franchise = pd.merge(df_franchise, df_address_full[['ID_LAMAL','geometry']].drop_duplicates(), on = 'ID_LAMAL')\n",
    "df_franchise = df_franchise.set_geometry('geometry')\n",
    "df_franchise = df_franchise.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnn16 = lps.weights.KNN(cKDTree(get_points_array(df_franchise.geometry)),50)\n",
    "# getiswnn16 = pyspace.compute_getis(df_franchise,'MTFRANCHISECOUV',wnn16, 999, p_001 = False)\n",
    "# fdr_pvalue = fdr(getiswnn16.p_sim, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181",
   "metadata": {},
   "source": [
    "### Distribution géographique du montant de la franchise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_franchise['MTFRANCHISECOUV_G_cl_fdr'] = df_franchise['MTFRANCHISECOUV_G_cl']  \n",
    "# df_franchise.loc[df_franchise['MTFRANCHISECOUV_G_psim'] >= fdr_pvalue, 'MTFRANCHISECOUV_G_cl_fdr'] = 'Not significant'\n",
    "# fig, ax = pyspace.plotGetisMap(df_franchise,'MTFRANCHISECOUV_G_cl',p_001 = False, commune_name = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df_couverture_aos[['ID_LAMAL','NBAGE']].groupby('ID_LAMAL', observed = True)['NBAGE'].max()\n",
    "df_age = pd.merge(df_age, df_address_full[['ID_LAMAL','geometry']].drop_duplicates(), on = 'ID_LAMAL')\n",
    "df_age = df_age.set_geometry('geometry')\n",
    "df_age = df_age.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df_age.sample(120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age['E'] = df_age['geometry'].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnn16 = lps.weights.KNN(cKDTree(get_points_array(df_age.geometry)),50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from esda.getisord import G_Local\n",
    "\n",
    "# g = G_Local(df_age['NBAGE'], wnn16, star = True, permutations = 999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getiswnn16 = pyspace.compute_getis(df_age,'NBAGE',wnn16, 999, p_001 = False)\n",
    "# fdr_pvalue = fdr(getiswnn16.p_sim, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_age['NBAGE_G_cl_fdr'] = df_age['NBAGE_G_cl']  \n",
    "# df_age.loc[df_age['NBAGE_G_psim'] >= fdr_pvalue, 'NBAGE_G_cl_fdr'] = 'Not significant'\n",
    "# fig, ax = pyspace.plotGetisMap(df_age,'NBAGE_G_cl',p_001 = False, commune_name = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
