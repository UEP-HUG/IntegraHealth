{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preparation for days at GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "import recordlinkage as rl\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "# import maskmypy\n",
    "from recordlinkage.preprocessing import clean, phonetic\n",
    "# from geofeather import to_geofeather, from_geofeather\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "from maskmypy import Donut\n",
    "import pickle\n",
    "# import contextily as ctx\n",
    "from geopandas import GeoDataFrame, sjoin\n",
    "\n",
    "data_folder = Path('../Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Base():\n",
    "    ''' Base class for masking methods '''\n",
    "    def __init__(self,\n",
    "                sensitive_gdf,\n",
    "                population_gdf='',\n",
    "                population_column='pop',\n",
    "                container_gdf='',\n",
    "                max_tries=1000,\n",
    "                address_points_gdf=''):\n",
    "        self.sensitive = sensitive_gdf.copy()\n",
    "        self.crs = self.sensitive.crs\n",
    "        self._load_population(population_gdf, population_column)\n",
    "        self._load_container(container_gdf)\n",
    "        self._load_addresses(address_points_gdf)\n",
    "        self.max_tries = max_tries\n",
    "\n",
    "\n",
    "    def _load_population(self, population_gdf='', population_column='pop'):\n",
    "        '''Loads a geodataframe of population data for donut masking \n",
    "        and/or k-anonymity estimation.'''\n",
    "        if isinstance(population_gdf, GeoDataFrame):\n",
    "            assert population_gdf.crs == self.crs, \"Population CRS does \"\\\n",
    "                \"not match points CRS\"\n",
    "            self.population = self._crop_gdf(\n",
    "                population_gdf, self.sensitive)\n",
    "            self.pop_column = population_column\n",
    "            self.population = self.population.loc[:,['geometry', self.pop_column]]\n",
    "            return True\n",
    "        else:\n",
    "            self.population = ''\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _load_container(self, container_gdf):\n",
    "        '''Loads a geodataframe of polygons to contain points while donut masking'''\n",
    "        if isinstance(container_gdf, GeoDataFrame):\n",
    "            assert container_gdf.crs == self.crs, \"Container CRS does \"\\\n",
    "                \"not match points CRS\"\n",
    "            self.container = self._crop_gdf(\n",
    "                container_gdf, self.sensitive)\n",
    "            self.container = self.container.loc[:,['geometry']]\n",
    "            self.container_filtered = self.container.copy()\n",
    "            return True\n",
    "        else:\n",
    "            self.container = ''\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _load_addresses(self, address_points_gdf):\n",
    "        '''Loads geodataframe containing address data for k-anonymity calculation'''\n",
    "        if isinstance(address_points_gdf, GeoDataFrame):\n",
    "            assert address_points_gdf.crs == self.crs, \"Address points \"\\\n",
    "                \"CRS does not match points CRS\"\n",
    "            self.addresses = self._crop_gdf(\n",
    "                address_points_gdf, self.sensitive)\n",
    "            self.addresses = self.addresses.loc[:,['geometry']]\n",
    "            return True\n",
    "        else:\n",
    "            self.addresses = ''\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _crop_gdf(self, target_gdf, reference_gdf):\n",
    "        '''Uses spatial index to reduce an input (target) geodataframe to only that which\n",
    "        intersects with a reference geodataframe'''\n",
    "        bb = reference_gdf.total_bounds\n",
    "        x = ((bb[2] - bb[0]) / 10)\n",
    "        y = ((bb[3] - bb[1]) / 10)\n",
    "        bb[0] = (bb[0] - x)\n",
    "        bb[1] = (bb[1] - y)\n",
    "        bb[2] = (bb[2] + x)\n",
    "        bb[3] = (bb[3] + y)\n",
    "        target_gdf = target_gdf.cx[bb[0]:bb[2], bb[1]:bb[3]]\n",
    "        return target_gdf\n",
    "\n",
    "\n",
    "    def displacement_distance(self):\n",
    "        '''Calculate dispalcement distance for each point after masking.'''\n",
    "        assert isinstance(self.masked, GeoDataFrame), \"Data has not yet been masked\"\n",
    "        for index, row in self.masked.iterrows():\n",
    "            old_coords = self.sensitive.at[index,'geometry']\n",
    "            distance = row.geometry.distance(old_coords)\n",
    "            self.masked.at[index,'distance'] = distance\n",
    "        return self.masked\n",
    "\n",
    "\n",
    "    def k_anonymity_estimate(self, population_gdf='', population_column='pop'):\n",
    "        '''Estimates k-anoynmity based on population data.'''\n",
    "        if not isinstance(self.population, GeoDataFrame):\n",
    "            self._load_population(population_gdf, population_column)\n",
    "\n",
    "        assert isinstance(self.sensitive, GeoDataFrame), \"Sensitive points geodataframe is missing\"\n",
    "        assert isinstance(self.masked, GeoDataFrame), \"Data has not yet been masked\"\n",
    "        assert isinstance(self.population, GeoDataFrame), \"Population geodataframe is missing\"\n",
    "\n",
    "        self.population['pop_area'] = self.population.area\n",
    "        \n",
    "        if 'distance' not in self.masked.columns:\n",
    "            self.displacement_distance()\n",
    "\n",
    "        masked_temp = self.masked.copy()\n",
    "\n",
    "        masked_temp['geometry'] = masked_temp.apply(\n",
    "            lambda x: x.geometry.buffer(x['distance']), axis=1)\n",
    "\n",
    "        masked_temp = self._disaggregate_population(masked_temp)\n",
    "\n",
    "        for i in range(len(self.masked.index)):\n",
    "            self.masked.at[i,'k_est'] = int(\n",
    "                masked_temp.loc[masked_temp['index_2'] == i, 'pop_adjusted'].sum() - 1)\n",
    "        \n",
    "        return self.masked\n",
    "\n",
    "\n",
    "    def k_anonymity_actual(self, address_points_gdf=''):\n",
    "        '''Calculates k-anonymity based on the number of addresses closer \n",
    "        to the masked point than sensitive point'''\n",
    "        if not isinstance(self.addresses, GeoDataFrame):\n",
    "            self._load_addresses(address_points_gdf)\n",
    "            \n",
    "        assert isinstance(self.sensitive, GeoDataFrame), \"Sensitive points geodataframe is missing\"\n",
    "        assert isinstance(self.masked, GeoDataFrame), \"Data has not yet been masked\"\n",
    "        assert isinstance(self.addresses, GeoDataFrame), \"Address points geodataframe is missing\"\n",
    "\n",
    "        if isinstance(self.addresses, GeoDataFrame) is False:\n",
    "            raise Exception(\"Error: missing address point geodataframe.\")\n",
    "\n",
    "        if 'distance' not in self.masked.columns:\n",
    "            self.displacement_distance()\n",
    "        \n",
    "        masked_temp = self.masked.copy()\n",
    "\n",
    "        masked_temp['geometry'] = masked_temp.apply(\n",
    "            lambda x: x.geometry.buffer(x['distance']), axis=1)\n",
    "\n",
    "        join = sjoin(self.addresses, masked_temp, how='left')\n",
    "\n",
    "        for i in range(len(self.masked)):\n",
    "            subset = join.loc[join['index_right'] == i,:]\n",
    "            self.masked.at[i,'k_actual'] = len(subset)\n",
    "\n",
    "        return self.masked\n",
    "\n",
    "\n",
    "    def _disaggregate_population(self, target_gdf):\n",
    "        '''Used for estimating k-anonymity. Disaggregates population within\n",
    "        buffers based on population polygon data'''\n",
    "        target = target_gdf.copy()\n",
    "        target = sjoin(\n",
    "            target, \n",
    "            self.population, \n",
    "            how='left')\n",
    "\n",
    "        target['index_2'] = target.index\n",
    "\n",
    "        target.index = range(len(target.index))\n",
    "\n",
    "        target['geometry'] = target.apply(\n",
    "            lambda x: x['geometry'].intersection(\n",
    "                self.population.at[x['index_right'],'geometry']), \n",
    "                axis=1)\n",
    "\n",
    "        target['intersected_area'] = target['geometry'].area\n",
    "\n",
    "        for i in range(len(target_gdf.index)):\n",
    "            \n",
    "            polygon_fragments = target.loc[target['index_2'] == i, :]\n",
    "            \n",
    "            for index, row in polygon_fragments.iterrows():\n",
    "                area_pct = row['intersected_area'] / row['pop_area']\n",
    "                target.at[index,'pop_adjusted'] = row[self.pop_column] * area_pct\n",
    "        \n",
    "        return target\n",
    "\n",
    "\n",
    "    def _containment(self, uncontained):\n",
    "        '''If a container geodataframe is loaded, checks whether or not masked \n",
    "        points are within the same containment polygon as their original locations.'''\n",
    "        if 'index_right' not in self.sensitive.columns:\n",
    "            self.sensitive = sjoin(self.sensitive, self.container, how='left')\n",
    "            self.tries = 0\n",
    "\n",
    "        self.container_filtered = self._crop_gdf(self.container_filtered, uncontained)\n",
    "\n",
    "        uncontained = sjoin(uncontained, self.container_filtered, how='left')\n",
    "\n",
    "        for index, row in uncontained.iterrows():\n",
    "            if row['index_right'] == self.sensitive.iat[index, -1]:\n",
    "                self.masked.at[index,'contain'] = 1\n",
    "\n",
    "        self.tries +=1\n",
    "        \n",
    "        if self.tries > self.max_tries:\n",
    "            for index, row in uncontained.iterrows():\n",
    "                self.masked.loc[index,'contain'] = 999\n",
    "            \n",
    "            print(str(len(uncontained)) + \" points were masked but could not be\" \\\n",
    "                \"contained. Uncontained points are listed as 999 in the 'contain' field\")\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame, sjoin\n",
    "from random import random, gauss, uniform\n",
    "from shapely.affinity import translate\n",
    "from math import sqrt\n",
    "\n",
    "class Donut(Base):\n",
    "\n",
    "    def __init__(\n",
    "                self, \n",
    "                sensitive_gdf, \n",
    "                population_gdf='', \n",
    "                population_column='pop',\n",
    "                max_distance=250,\n",
    "                donut_ratio=0.1,\n",
    "                distribution='uniform',\n",
    "                container_gdf='',\n",
    "                address_points_gdf='',\n",
    "                max_tries=1000):\n",
    "        \n",
    "        super().__init__(\n",
    "            sensitive_gdf = sensitive_gdf, \n",
    "            population_gdf = population_gdf, \n",
    "            population_column = population_column,\n",
    "            container_gdf = container_gdf,\n",
    "            max_tries = max_tries,\n",
    "            address_points_gdf = address_points_gdf)\n",
    "\n",
    "        self.max = max_distance\n",
    "        self.distribution = distribution\n",
    "        self.donut_ratio = donut_ratio\n",
    "        \n",
    "\n",
    "    def _random_xy(self, min, max):\n",
    "        if self.distribution == 'uniform':\n",
    "            hypotenuse = uniform(min, max)\n",
    "            x = uniform(0,hypotenuse)\n",
    "\n",
    "        elif self.distribution == 'gaussian':\n",
    "            mean = (((max - min) / 2) + min)\n",
    "            sigma = (((max - min) / 2) / 2.5)\n",
    "            hypotenuse = gauss(mean, sigma)\n",
    "            x = uniform(0, hypotenuse)\n",
    "\n",
    "        elif self.distribution == 'areal':\n",
    "            hypotenuse = 0\n",
    "            while hypotenuse == 0:\n",
    "                r1 = uniform(min, max)\n",
    "                r2 = uniform(min, max)\n",
    "                if r1 > r2:\n",
    "                    hypotenuse = r1\n",
    "            x = uniform(0, hypotenuse)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Unknown distribution\") \n",
    "\n",
    "        y = sqrt(hypotenuse**2 - x**2)\n",
    "        \n",
    "        direction = random()\n",
    "\n",
    "        if direction < 0.25:\n",
    "            x = x * -1\n",
    "\n",
    "        elif direction < 0.5:    \n",
    "            y = y * -1\n",
    "\n",
    "        elif direction < 0.75:\n",
    "            x = x * -1\n",
    "            y = y * -1\n",
    "\n",
    "        elif direction < 1:\n",
    "            pass\n",
    "\n",
    "        return (x, y)\n",
    "    \n",
    "\n",
    "    def _find_radii(self):\n",
    "        self.masked.loc[:,'radius_min'] = self.max * self.donut_ratio\n",
    "        self.masked.loc[:,'radius_max'] = self.max\n",
    "\n",
    "\n",
    "    def _mask_within_container(self): \n",
    "        self.masked.loc[:,'contain'] = 0  \n",
    "        \n",
    "        while min(self.masked['contain']) == 0:\n",
    "            \n",
    "            uncontained = self.masked.loc[self.masked['contain'] == 0, :]\n",
    "            \n",
    "            for index, row in uncontained.iterrows():\n",
    "                x,y = self._random_xy(row['radius_min'], row['radius_max'])\n",
    "                \n",
    "                self.masked.at[index, 'geometry'] = translate(\n",
    "                    row['geometry'], xoff=x, yoff=y)\n",
    "            \n",
    "            self._containment(uncontained)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def execute(self):\n",
    "        self.masked = self.sensitive.copy()\n",
    "\n",
    "        self._find_radii()\n",
    "\n",
    "        self.masked['offset'] = self.masked.apply(\n",
    "            lambda x: self._random_xy(\n",
    "                x['radius_min'], x['radius_max']), axis=1)\n",
    "\n",
    "        self.masked['geometry'] = self.masked.apply(\n",
    "            lambda x: translate(\n",
    "                x['geometry'], xoff=x['offset'][0], yoff=x['offset'][1]), axis=1)\n",
    "\n",
    "        if isinstance(self.container, GeoDataFrame):\n",
    "            self._mask_within_container()\n",
    "\n",
    "        self.masked = self.masked.drop(['offset'], axis=1)\n",
    "        \n",
    "        return self.masked\n",
    "\n",
    "\n",
    "\n",
    "class Donut_MaxK(Donut):\n",
    "\n",
    "    def __init__(\n",
    "                self, \n",
    "                sensitive_gdf, \n",
    "                population_gdf='', \n",
    "                population_column='pop',\n",
    "                max_k_anonymity=0,\n",
    "                donut_ratio=0.1,\n",
    "                distribution='uniform',\n",
    "                container_gdf='',\n",
    "                address_points_gdf='',\n",
    "                max_tries=1000):\n",
    "        \n",
    "        super().__init__(\n",
    "            sensitive_gdf = sensitive_gdf, \n",
    "            population_gdf = population_gdf, \n",
    "            population_column = population_column,\n",
    "            container_gdf = container_gdf,\n",
    "            max_tries = max_tries,\n",
    "            address_points_gdf = address_points_gdf,\n",
    "            donut_ratio = donut_ratio,\n",
    "            distribution = distribution)\n",
    "\n",
    "        self.target_k = max_k_anonymity\n",
    "\n",
    "\n",
    "    def _find_radii(self): \n",
    "\n",
    "        self.population['pop_area'] = self.population.area\n",
    "\n",
    "        join = sjoin(self.masked, self.population, how='left')\n",
    "        \n",
    "        join['max_area'] = join.apply(\n",
    "            lambda x: self.target_k * x['pop_area'] / x[self.pop_column], axis=1)\n",
    "\n",
    "        join['min_area'] = join.apply(\n",
    "            lambda x: (self.target_k * self.donut_ratio) * x['pop_area'] / \n",
    "                x[self.pop_column], axis=1)\n",
    "\n",
    "        join['max_radius'] = join.apply(\n",
    "            lambda x: sqrt(x['max_area'] / 3.141592654), axis=1)\n",
    "\n",
    "        join['min_radius'] = join.apply(\n",
    "            lambda x: sqrt(x['min_area'] / 3.141592654), axis=1)\n",
    "        \n",
    "        self.masked['radius_min'] = join.apply(\n",
    "            lambda x: x['min_radius'], axis=1)\n",
    "        self.masked['radius_max'] = join.apply(\n",
    "            lambda x: x['max_radius'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "class Donut_Multiply(Donut):\n",
    "\n",
    "    def __init__(\n",
    "                self, \n",
    "                sensitive_gdf, \n",
    "                max_distance=250,\n",
    "                population_gdf='', \n",
    "                population_column='pop',\n",
    "                population_multiplier = 0,\n",
    "                donut_ratio=0.1,\n",
    "                distribution='uniform',\n",
    "                container_gdf='',\n",
    "                address_points_gdf='',\n",
    "                max_tries=1000):\n",
    "        \n",
    "        super().__init__(\n",
    "            sensitive_gdf = sensitive_gdf, \n",
    "            max_distance = max_distance,\n",
    "            population_gdf = population_gdf, \n",
    "            population_column = population_column,\n",
    "            container_gdf = container_gdf,\n",
    "            max_tries = max_tries,\n",
    "            address_points_gdf = address_points_gdf,\n",
    "            donut_ratio = donut_ratio,\n",
    "            distribution = distribution,\n",
    "                )\n",
    "\n",
    "        self.pop_multiplier = population_multiplier - 1\n",
    "        \n",
    "\n",
    "    def _find_radii(self):\n",
    "        self.population['pop_area'] = self.population.area\n",
    "\n",
    "        join = sjoin(self.masked, self.population, how='left')\n",
    "        \n",
    "        pop_min = min(join[self.pop_column])\n",
    "        pop_max = max(join[self.pop_column])\n",
    "        pop_range =  pop_max - pop_min\n",
    "        \n",
    "        join['pop_score'] = join.apply(\n",
    "            lambda x: (1 - (x[self.pop_column] - pop_min) / pop_range) \\\n",
    "                * self.pop_multiplier, axis=1)\n",
    "\n",
    "        self.masked['radius_max'] = join.apply(\n",
    "            lambda x: (x['pop_score'] * self.max) + self.max, axis=1)\n",
    "\n",
    "        self.masked['radius_min'] = self.masked.apply(\n",
    "            lambda x: x['radius_max'] * self.donut_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nationalities = pd.read_csv('/Users/david/Dropbox/PhD/GitHub/SanteIntegra/Data/CH_Nationality_List_20171130_v1.csv')\n",
    "# # patients_delta = pd.read_feather('/Users/david/Dropbox/PhD/GitHub/deltagiraph/data/preprocessed/patient_01.01.2019-31.12.2019.feather') \n",
    "# geom_delta = from_geofeather('/Users/david/Dropbox/PhD/GitHub/deltagiraph/data/preprocessed/geometries_01.01.2019-31.12.2019.feather')   \n",
    "# patients_delta = from_geofeather('/Users/david/Dropbox/PhD/GitHub/deltagiraph/data/preprocessed/patient_01.01.2019-31.12.2019.feather')    \n",
    "# patients_delta['nationality'] = np.random.randint(1, 225, patients_delta.shape[0])\n",
    "# patients_delta['nationality'] = patients_delta['nationality'].apply(lambda x: nationalities.loc[x])\n",
    "# patient_gm = patients_delta[patients_delta.insurance == 'Groupe Mutuel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lca = pd.read_csv(data_folder/'SMG_RES_LCA_FOR_LINK_ok.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal = pd.read_csv(data_folder/'SMG_RECORD_LINKAGE_LAMal_OK.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_patient_gm = patient_gm[['lon','lat','sex','address_id','age','nationality','geometry']][patient_gm.lon.isnull()==False].sample(1000).reset_index()\n",
    "# dummy_patient_gm = dummy_patient_gm.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_patient_gm = pd.merge(dummy_patient_gm,geom_delta[['address','address_id']],on = 'address_id',how = 'left').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_patient_gm = dummy_patient_gm.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Prepare geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champs_address = ['TXRUELEGALE', 'TXRUENUMEROLEGALE', 'TXNPALEGALE', 'TXLOCALITELEGALE', 'TXCOMPLEMENTDESTLEGALE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gdf(df,crs,x,y):\n",
    "    geometry = [Point(xy) for xy in zip(df[x], df[y])]\n",
    "    crs ='epsg:{}'.format(crs)\n",
    "    gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "champ_dict = {'NOANNEE':'NOANNEE','ID_LCA':'ID_LCA','ANNEE_NAISSANCE':'ANNEE_NAISSANCE','mois_mod2':'MOIS_NAISSANCE','CDPHYSSEXE':'SEXE','CDPHYSNATIONALITE':'NATION','TXCOMPLEMENTDESTLEGALE':'COMP_DEST_LEGAL','TXRUELEGALE':'street','TXRUENUMEROLEGALE':'adr_num','TXNPALEGALE':'zipcode','TXLOCALITELEGALE':'city'}\n",
    "champ_dict_lamal = {'NOANNEE':'NOANNEE','ID_LAMal':'ID_LAMAL','Annee_naiss':'ANNEE_NAISSANCE','mois_mod2':'MOIS_NAISSANCE','CDPHYSSEXE':'SEXE','CDPHYSNATIONALITE':'NATION','TXCOMPLEMENTDESTLEGALE':'COMP_DEST_LEGAL','TXRUELEGALE':'street','TXRUENUMEROLEGALE':'adr_num','TXNPALEGALE':'zipcode','TXLOCALITELEGALE':'city'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lca.columns = lca.columns.map(champ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal.columns = lamal.columns.map(champ_dict_lamal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statpop = pd.read_csv('/Users/david/Dropbox/PhD/Data/Databases/OFS/ag-b-00.03-vz2019statpop/STATPOP2019.csv')\n",
    "# geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(statpop.E_KOORD, statpop.N_KOORD)]\n",
    "# statpop_gdf = gpd.GeoDataFrame(statpop, crs=2056, geometry=geometry) \n",
    "regbl_address = pd.read_pickle(data_folder/'2020_regbl_address.pkl')\n",
    "regbl_address[['gkode','gkodn']] = regbl_address[['gkode','gkodn']].astype(float)\n",
    "regbl_address = make_gdf(regbl_address,'2056','gkode','gkodn')\n",
    "regbl_address['address'] = regbl_address['address'].str[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "regbl_address = regbl_address[regbl_address.is_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    if text is not None:\n",
    "        for ch in ['\\r','\\n','.','/']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch,'')\n",
    "        for ch in ['ã®']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch,'î')\n",
    "        for ch in ['ã»']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch,'û')\n",
    "        for ch in ['c/o','\\r','\\n','c.f.','chez']:\n",
    "            if ch in text.split(' '):\n",
    "                text = text.replace(ch,\" \")\n",
    "        if 'ch.' in text.split(' '):\n",
    "            text = text.replace('ch. ',\"chemin \")\n",
    "        elif 'ch' in text.split(' '):\n",
    "            text = text.replace('ch ',\"chemin \")\n",
    "        elif 'rte.' in text.split(' '):\n",
    "            text = text.replace('rte. ',\"route \")\n",
    "        elif 'rte' in text.split(' '):\n",
    "            text = text.replace('rte ',\"route \")    \n",
    "        elif 'av.' in text:\n",
    "            text = text.replace('av. ',\"avenue \")\n",
    "        elif 'av' in text:\n",
    "            text = text.replace('av ',\"avenue \")\n",
    "        elif 'bd' in text:\n",
    "            text = text.replace('bd ',\"boulevard \")\n",
    "        elif 'bd. ' in text:\n",
    "            text = text.replace('bd. ',\"boulevard \")\n",
    "        elif 'bvd' in text:\n",
    "            text = text.replace('bvd',\"boulevard \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file =  data_folder/ \"geocoding_cache.pkl\"\n",
    "if cache_file.exists():\n",
    "    with open(cache_file, 'rb') as file:\n",
    "        cache = pickle.load(file)\n",
    "else:\n",
    "    cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from numbers import Number\n",
    "\n",
    "def _cache_key(row):\n",
    "    adr_num = row['adr_num']\n",
    "    if isinstance(adr_num, Number) and math.isnan(adr_num):\n",
    "        adr_num = None\n",
    "    return row['zipcode'], adr_num, row['full_address']\n",
    "def geocoding_cache(row):\n",
    "    key = _cache_key(row)\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    else:\n",
    "        output = geocoding(row)\n",
    "        cache[key] = output\n",
    "        return output\n",
    "\n",
    "def run_geocoding_cache(df):\n",
    "    try:\n",
    "        return run_geocoding(df)\n",
    "    finally:\n",
    "        with open(cache_file, 'wb') as file:\n",
    "            pickle.dump(cache, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def run_geocoding(df):\n",
    "    v0s, v1s, v2s, v3s, v4s, v5s = [], [], [], [], [], []\n",
    "    for index, row in df.reset_index(drop = True).iterrows():\n",
    "        if (index + 1) % 500 == 0 or math.remainder(math.log10(index + 1), 1) == 0 or index + 1 == len(df):\n",
    "            print(\"geocoding item\",index + 1, \"of \", len(df))\n",
    "        v1, v2, v3, v4, v5 = geocoding_cache(row)\n",
    "        v0s.append(row.full_address)\n",
    "        v1s.append(v1)\n",
    "        v2s.append(v2)\n",
    "        v3s.append(v3)\n",
    "        v4s.append(v4)\n",
    "        v5s.append(v5)\n",
    "    df_result = pd.DataFrame({'full_address': v0s,\n",
    "                              'similarity': v1s,\n",
    "                              'new_address': v2s,\n",
    "                              'E': v3s,\n",
    "                              'N': v4s,\n",
    "                              'comment': v5s})\n",
    "    return df_result\n",
    "def geocoding(row):\n",
    "    v1, v2, v3, v4, v5 = geocoding_quick(row)\n",
    "#     if v3 == 0 or v3 is None or v1 < 90:\n",
    "#         v1, v2, v3, v4, v5 = geocoding_slow(row)\n",
    "    return v1, v2, v3, v4, v5\n",
    "def similar(a, b):  # Returns the percentage of matching sequence between 2 strings\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "def geocoding_quick(row):\n",
    "    max_value = 0\n",
    "    dict_values = {}\n",
    "    for id,line in regbl_address[(regbl_address['plz4']==row['zipcode'])&(regbl_address['deinr']==row['adr_num'])].iterrows():\n",
    "        value = similar(row.full_address,line.address)*100\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            # log.info('Geocoding:\\n%s\\n%s',row.full_address,line.address)\n",
    "            dict_values[max_value] = [line.address,line.gkode,line.gkodn,'']\n",
    "    if max_value == 0:\n",
    "        dict_values[0] = [None,None,None,'No match']\n",
    "    max_key = list(max(dict_values.items(), key=lambda k: k[0]))\n",
    "    similarity,new_address,E,N,comment = max_key[0], max_key[1][0], max_key[1][1], max_key[1][2], max_key[1][3]\n",
    "    return similarity,new_address,E,N,comment\n",
    "def geocoding_slow(row):\n",
    "    max_value = 0\n",
    "    dict_values = {}\n",
    "    filtered_df = regbl_address[(regbl_address['deinr'].astype(str)==str(row['adr_num']))&(regbl_address['gdekt']==row['canton'])].sort_values('address')\n",
    "    if filtered_df.empty:\n",
    "        filtered_df = regbl_address[(regbl_address['gdekt']==row['canton'])].sort_values('address')\n",
    "        filtered_df = filtered_df[(filtered_df['deinr'].astype(str)==str(row['adr_num']))|(filtered_df['plz4']==row['zipcode'])|(filtered_df['plz4']==row['zipcode'])]\n",
    "\n",
    "    for id,line in filtered_df.iterrows():\n",
    "        value = similar(row.full_address,line.address)*100\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            # log.info('Slow geocoding:\\n%s\\n%s',row.full_address,line.address)\n",
    "            dict_values[max_value] = [line.address,line.gkode,line.gkodn,'']\n",
    "    if max_value == 0:\n",
    "        dict_values[0] = [None,None,None,'No match']\n",
    "    max_key = list(max(dict_values.items(), key=lambda k: k[0]))\n",
    "    similarity,new_address,E,N,comment = max_key[0], max_key[1][0], max_key[1][1], max_key[1][2], max_key[1][3]\n",
    "    return similarity,new_address,E,N,comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if regbl_address_df is None:\n",
    "#     regbl_address = pd.read_pickle(data_folder/'regbl_address.pkl')\n",
    "# regbl_address = regbl_address_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adr_num(street_name):\n",
    "    \"Extract the street number from a street name\"\n",
    "    if street_name is not np.nan:\n",
    "        try:\n",
    "            adr_num = [int(s) for s in street_name.split()[-1] if s.isdigit()]\n",
    "            if len(adr_num) > 0 :\n",
    "                return street_name.split()[-1]\n",
    "        except:\n",
    "            print(\"invalid street name: %s\", street_name)\n",
    "#             log.warning(\"invalid street name: %s\", street_name)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(db,no_rue_col,rue_col,ville_col,cp_col,suffix):\n",
    "    df = db.copy()\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    comment_col = 'comment'+ suffix\n",
    "    new_address = 'new_address'+ suffix\n",
    "    no_rue = no_rue_col+ suffix\n",
    "    \n",
    "    ## Add new columns\n",
    "    df[comment_col] = ''\n",
    "    df[new_address] = ''\n",
    "    \n",
    "    ##Zipcodes\n",
    "    #1 Exclude zipcodes outside of CH\n",
    "    df = df[df[cp_col].astype(str).str.contains('[A-Za-z]', regex= True, na=False) == False]\n",
    "    df = df[df[cp_col].astype(str).str.contains('[\\s]', regex= True, na=False) == False]\n",
    "    df = df[df[cp_col].astype(str).str.contains('[-]', regex= True, na=False) == False]\n",
    "    #2 Repare zipcode field : Going from 1201.0 to integer 1201 to '1201'\n",
    "    df['zipcode'] = (df[cp_col].fillna(-1) #Replace all NAs to -1\n",
    "    .astype(int) #Convert to integer\n",
    "    .astype(str) #Convert to string\n",
    "    .replace('-1',np.nan)) #Convert '-1' to NAs\n",
    "    df[cp_col] = df[cp_col].astype(str)\n",
    "    df[cp_col] = df[cp_col].replace('\\.','',regex = True)\n",
    "    \n",
    "    ##Streets\n",
    "    df = df[df[rue_col].str.isspace() == False]\n",
    "    \n",
    "    ##Address numbers\n",
    "#     df[no_rue] = df[df[rue_col].isnull()==False].apply(lambda x:get_adr_num(x[rue_col]),axis = 1) #Extract adress number from street column\n",
    "    df[no_rue] = df[no_rue].fillna(np.nan) #Fill NA\n",
    "    df[no_rue] = df[no_rue].replace('\\.','',regex = True)\n",
    "    df[no_rue] = df[no_rue].replace('/','').str.strip() #Replace unwanted '/' character with ''\n",
    "    df[no_rue] = df[no_rue].str.split(' ').str[0] #Get\n",
    "    df[no_rue] = df[no_rue].str.split('-').str[0] #Get\n",
    "    df[no_rue]= df[no_rue].str.lstrip('0')\n",
    "    df[no_rue] = df[no_rue].str.lower()\n",
    "    ##City\n",
    "    df[ville_col] = df[ville_col].replace('\\.','',regex = True)\n",
    "\n",
    "    ##Whole\n",
    "    df[rue_col] = df[rue_col].astype(str)\n",
    "    # df[no_rue] = df[no_rue].astype(str)\n",
    "#     df[canton_col] = df[canton_col].astype(str).str.lower()\n",
    "    df = df.replace(r'^\\s*$', np.NaN, regex=True)\n",
    "    df.loc[df[no_rue] == 'nan', no_rue] = np.nan #Update fields == 'nan' to NaN\n",
    "    full_address_col = 'full_address'+suffix\n",
    "    df[full_address_col] = df[rue_col].fillna('').astype(str).str.lower()+ ' ' + df[no_rue].fillna('').astype(str).str.lower() +' '+df[cp_col].fillna('').astype(str)  + ' '+ df[ville_col].astype(str).str.lower()\n",
    "    df[full_address_col] = df.apply(lambda x: filter_text(x[full_address_col]), axis=1)\n",
    "    print('******Addresses cleaned successfully******')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_post_georef(db,df_geom,qc_cutoff,no_rue_col,rue_col,ville_col,cp_col,suffix):\n",
    "    df = db.copy()\n",
    "    df.drop(['new_address'],axis = 1,inplace = True)\n",
    "    df = df.rename(columns = {'gkode':\"E\",'gkodn':'N'})\n",
    "    full_address_col = 'full_address'+suffix\n",
    "    comment_col = 'comment'+ suffix\n",
    "    comment_col_x = comment_col+'_x'\n",
    "    comment_col_y = comment_col+'_y'\n",
    "    E_col_x, E_col_y = 'E_x', 'E_y'\n",
    "    N_col_x, N_col_y = 'N_x', 'N_y'\n",
    "    no_rue_col = no_rue_col+suffix\n",
    "    new_address_col = 'new_address'+suffix\n",
    "    df = df.merge(df_geom, on=full_address_col, how='left')\n",
    "    df[comment_col] = df[comment_col_x].fillna('') + df[comment_col_y].fillna('')\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df['E'] = df[E_col_x].fillna(0).astype(float) + df[E_col_y].fillna(0).astype(float)\n",
    "    df['N'] = df[N_col_x].fillna(0).astype(float) + df[N_col_y].fillna(0).astype(float)\n",
    "    df.drop([E_col_x, E_col_y, N_col_x, N_col_y, comment_col_x, comment_col_y], axis=1, inplace=True)\n",
    "    df.loc[df.E == 0,'E'] = np.nan\n",
    "    df.loc[df.N == 0,'N'] = np.nan\n",
    "    df.loc[df[comment_col] == 'nan', comment_col] = np.nan\n",
    "    df.loc[df[comment_col].isnull() == True, comment_col] = 'Ok'\n",
    "    df.loc[(df.similarity < qc_cutoff) & (df[comment_col].isnull() == False), comment_col] = 'Low quality'\n",
    "    df.loc[(df.similarity == 0) & (df[comment_col].isnull() == False), comment_col] = 'No match'\n",
    "    df.loc[df[no_rue_col].isnull(),comment_col] = 'No street number'\n",
    "    df.loc[df[ville_col].isnull(), comment_col] = 'No address'\n",
    "    quality_distrib = df[[rue_col,no_rue_col,ville_col,cp_col,full_address_col,new_address_col,comment_col]].groupby(comment_col)[rue_col].count()\n",
    "    print(quality_distrib)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Geocoding LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom = clean_data(lca,'adr_num','street','city','zipcode','')\n",
    "\n",
    "df_geom['geom_id'] = df_geom.index+1\n",
    "\n",
    "df_geom = pd.merge(df_geom[['ID_LCA','geom_id','street','adr_num','city','zipcode','full_address','new_address','comment']],regbl_address[['deinr','plz4','gdekt','address','gkode','gkodn']],how = 'left', right_on = 'address',left_on = 'full_address').drop('address',axis = 1)\n",
    "\n",
    "to_geocode = df_geom[(df_geom.adr_num != 'nan')&(df_geom.gkode.isnull())][['full_address','zipcode','adr_num']].drop_duplicates()\n",
    "\n",
    "geocoded = run_geocoding_cache(to_geocode)\n",
    "\n",
    "processed_df = clean_post_georef(df_geom,geocoded,85,'adr_num','street','city','zipcode','')\n",
    "\n",
    "processed_df = make_gdf(processed_df,2056,'E','N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(data_folder/'Clean_data'/'geocoded_lca_all.csv',index = False)\n",
    "\n",
    "processed_df_nonull = processed_df[(processed_df.E.isnull()==False) & (processed_df.N.isnull()==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_nonull.ID_LCA.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Geocoding LAMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom_lamal = clean_data(lamal,'adr_num','street','city','zipcode','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom_lamal_in_lca = df_geom_lamal[df_geom_lamal.full_address.isin(df_geom.full_address)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom_lamal_in_lca = df_geom_lamal_in_lca.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom_lamal_in_lca['geom_id'] = df_geom_lamal_in_lca.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geom_lamal_in_lca = pd.merge(df_geom_lamal_in_lca[['ID_LAMAL','geom_id','street','adr_num','city','zipcode','full_address','new_address','comment']],regbl_address[['deinr','plz4','gdekt','address','gkode','gkodn']],how = 'left', right_on = 'address',left_on = 'full_address').drop('address',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_geocode_lamal = df_geom_lamal_in_lca[(df_geom_lamal_in_lca.adr_num != 'nan')&(df_geom_lamal_in_lca.gkode.isnull())][['full_address','zipcode','adr_num']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_lamal = run_geocoding_cache(to_geocode_lamal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_lamal = clean_post_georef(df_geom_lamal_in_lca,geocoded_lamal,85,'adr_num','street','city','zipcode','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_lamal = make_gdf(processed_df_lamal,2056,'E','N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_lamal.to_csv(data_folder/'Clean_data'/'geocoded_lamal_all.csv',index = False)\n",
    "processed_df_lamal_nonull = processed_df_lamal[(processed_df_lamal.E.isnull()==False) & (processed_df_lamal.N.isnull()==False)]\n",
    "processed_df_lamal_nonull = processed_df_lamal_nonull[processed_df_lamal_nonull.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_lamal_nonull.ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Prepare geomasking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Population-based donut geomasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "communes = gpd.read_file(data_folder/'SHAPEFILE_LV95_LN02'/'swissBOUNDARIES3D_1_3_TLM_HOHEITSGEBIET.shp')\n",
    "communes = communes[~communes.geometry.isnull()]\n",
    "communes = communes.rename(columns={'geom': 'geometry'})\n",
    "communes = communes[communes.NAME != 'Lac Léman (VD)']\n",
    "communes = communes[communes.NAME != 'Lac de Neuchâtel (VD)']\n",
    "communes = communes[communes.NAME != 'Lac de Morat (VD)']\n",
    "communes = communes.reset_index(drop=True)\n",
    "communes = gpd.GeoDataFrame(communes, crs = 2056,geometry=communes['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_3D_2D(geometry):\n",
    "    '''\n",
    "    Takes a GeoSeries of 3D Multi/Polygons (has_z) and returns a list of 2D Multi/Polygons\n",
    "    '''\n",
    "    new_geo = []\n",
    "    for p in geometry:\n",
    "        if p.has_z:\n",
    "            if p.geom_type == 'Polygon':\n",
    "                lines = [xy[:2] for xy in list(p.exterior.coords)]\n",
    "                new_p = Polygon(lines)\n",
    "                new_geo.append(new_p)\n",
    "            elif p.geom_type == 'MultiPolygon':\n",
    "                new_multi_p = []\n",
    "                for ap in p:\n",
    "                    lines = [xy[:2] for xy in list(ap.exterior.coords)]\n",
    "                    new_p = Polygon(lines)\n",
    "                    new_multi_p.append(new_p)\n",
    "                new_geo.append(MultiPolygon(new_multi_p))\n",
    "    return new_geo\n",
    "\n",
    "communes['geometry'] = convert_3D_2D(communes['geometry'])\n",
    "\n",
    "communes.crs = 2056"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Geomasking - LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_nonull = processed_df_nonull.drop('geom_id',axis =1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "donutmask = Donut(\n",
    "    sensitive_gdf=processed_df_nonull, # Name of the sensitive geodataframe\n",
    "    population_gdf=communes, # Name of the census geodataframe\n",
    "    population_column = 'EINWOHNERZ',\n",
    "    max_distance=250, # The maximum possible distance that points are displaced\n",
    "    donut_ratio=0.1, # The ratio used to define the minimum distance points are displaced\n",
    "    distribution='uniform' # The distribution to use when displacing points. Other options include 'gaussian' and 'areal'. 'Areal' distribution means points are more likely to be displaced further within the range.\n",
    ") # Optional, a geodataframe used to ensure that points do not leave a particular area. \n",
    "\n",
    "donutmask.execute()\n",
    "\n",
    "masked_gdf = donutmask.displacement_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# donutmask.k_anonymity_actual(address_points_gdf=regbl_address) # Name of the geodataframe including address points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_gdf['lon_masked'] = masked_gdf['geometry'].to_crs(4326).x\n",
    "masked_gdf['lat_masked'] = masked_gdf['geometry'].to_crs(4326).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_gdf['address'] = masked_gdf['full_address']\n",
    "masked_gdf.loc[masked_gdf.new_address.isnull()==False, 'address'] = masked_gdf.new_address\n",
    "masked_gdf['address'] = pd.Categorical(masked_gdf['address'])\n",
    "masked_gdf['address_id'] = masked_gdf['address'].cat.codes.astype(int)\n",
    "masked_gdf['address'] = masked_gdf['address'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "masked_gdf.to_csv(data_folder/'Clean_data'/'masked_lca_nonull.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of NATION duplicates by aggregating, CH if contains CH, otherwise max\n",
    "agg_nation = lca.groupby('ID_LCA')['NATION'].apply(lambda x: 'CH' if 'CH' in x else max(x))\n",
    "\n",
    "lca['NATION_NODUP'] = lca['ID_LCA'].map(agg_nation.to_dict())\n",
    "\n",
    "#Get rid of SEXE duplicates by aggregating, max\n",
    "agg_sexe = lca.groupby('ID_LCA')['SEXE'].apply(lambda x: max(x))\n",
    "\n",
    "lca['SEXE_NODUP'] = lca['ID_LCA'].map(agg_sexe.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_mask_link = pd.merge(lca[['ID_LCA','ANNEE_NAISSANCE','MOIS_NAISSANCE','SEXE_NODUP','NATION_NODUP']].drop_duplicates(),masked_gdf[['ID_LCA','lon_masked','lat_masked','zipcode','address_id']], on = 'ID_LCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_mask_link= lca_mask_link.sort_values('lon_masked').drop_duplicates(subset= ['ID_LCA','address_id'],keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Geomasking - LAMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_lamal_nonull = processed_df_lamal_nonull.drop('geom_id',axis =1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from numbers import Number\n",
    "import math\n",
    "import pickle\n",
    "import uuid\n",
    "# Import\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "donutmask = Donut(\n",
    "    sensitive_gdf=processed_df_lamal_nonull, # Name of the sensitive geodataframe\n",
    "    population_gdf=communes, # Name of the census geodataframe\n",
    "    population_column = 'EINWOHNERZ',\n",
    "    max_distance=250, # The maximum possible distance that points are displaced\n",
    "    donut_ratio=0.1, # The ratio used to define the minimum distance points are displaced\n",
    "    distribution='uniform' # The distribution to use when displacing points. Other options include 'gaussian' and 'areal'. 'Areal' distribution means points are more likely to be displaced further within the range.\n",
    ") # Optional, a geodataframe used to ensure that points do not leave a particular area. \n",
    "\n",
    "donutmask.execute()\n",
    "\n",
    "masked_gdf_lamal = donutmask.displacement_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# donutmask.k_anonymity_actual(address_points_gdf=regbl_address) # Name of the geodataframe including address points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_gdf_lamal['lon_masked'] = masked_gdf_lamal['geometry'].to_crs(4326).x\n",
    "masked_gdf_lamal['lat_masked'] = masked_gdf_lamal['geometry'].to_crs(4326).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_gdf_lamal.to_csv(data_folder/'Clean_data'/'masked_lamal_nonull.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_address_id = masked_gdf[['address','address_id']].set_index('address').to_dict()['address_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_address_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_gdf_lamal['address'] = masked_gdf_lamal['full_address']\n",
    "masked_gdf_lamal.loc[masked_gdf_lamal.new_address.isnull()==False, 'address'] = masked_gdf_lamal.new_address\n",
    "masked_gdf_lamal['address_id'] = masked_gdf_lamal['address'].map(key_address_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal['NATION'] = lamal['NATION'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of NATION duplicates by aggregating, CH if contains CH, otherwise max\n",
    "agg_nation = lamal.groupby('ID_LAMAL')['NATION'].apply(lambda x: 'CH' if 'CH' in x else max(x))\n",
    "lamal['NATION_NODUP'] = lamal['ID_LAMAL'].map(agg_nation.to_dict())\n",
    "#Get rid of SEXE duplicates by aggregating, max\n",
    "agg_sexe = lamal.groupby('ID_LAMAL')['SEXE'].apply(lambda x: max(x))\n",
    "lamal['SEXE_NODUP'] = lamal['ID_LAMAL'].map(agg_sexe.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal_mask_link = pd.merge(lamal[['ID_LAMAL','ANNEE_NAISSANCE','MOIS_NAISSANCE','SEXE_NODUP','NATION_NODUP']].drop_duplicates(),masked_gdf_lamal[['ID_LAMAL','lon_masked','lat_masked','zipcode','address_id']], on = 'ID_LAMAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal_mask_link = lamal_mask_link.sort_values('lon_masked').drop_duplicates(subset= ['ID_LAMAL','address_id'],keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal.ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_mask_link.ID_LCA.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_mask_link.to_csv(data_folder/'Clean_data'/'lca_masked_for_linkage.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal_mask_link.to_csv(data_folder/'Clean_data'/'lamal_masked_for_linkage.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamal_mask_link.ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### Street-based geomasking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "Much too slow to implement for our purpose (~ 100,000 addresses) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## Prepare record linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = rl.Index()\n",
    "#champ_dict = {'NOANNEE':'NOANNEE','ID_LCA':'ID_LCA','ANNEE_NAISSANCE':'ANNEE_NAISSANCE','mois_mod2':'MOIS_NAISSANCE','CDPHYSSEXE':'SEXE','CDPHYSNATIONALITE':'NATION','TXCOMPLEMENTDESTLEGALE':'COMP_DEST_LEGAL','TXRUELEGALE':'street','TXRUENUMEROLEGALE':'adr_num','TXNPALEGALE':'zipcode','TXLOCALITELEGALE':'city'}\n",
    "indexer.block(['ANNEE_NAISSANCE','MOIS_NAISSANCE','SEXE_NODUP','NATION_NODUP','zipcode'])\n",
    "pairs = indexer.index(lca_mask_link, lamal_mask_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(lca_mask_link), len(lamal_mask_link), len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 6 #Set number of CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer = rl.Compare(n_jobs=n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#250m (max geomasking) - 25min (min) / 2 (0.5 decay of the linear fct)\n",
    "#Because it is the min distance of geomasking\n",
    "# comparer.exact('given_name', 'given_name', label='given_name')\n",
    "# comparer.string('surname', 'surname', method='jarowinkler', threshold=0.85, label='surname')\n",
    "# comparer.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "comparer.string('NATION_NODUP', 'NATION_NODUP',method='jarowinkler', threshold=0.85, label='NATION')\n",
    "comparer.exact('ANNEE_NAISSANCE', 'ANNEE_NAISSANCE', label='ANNEE_NAISSANCE')\n",
    "comparer.exact('MOIS_NAISSANCE', 'MOIS_NAISSANCE', label='MOIS_NAISSANCE')\n",
    "comparer.exact('SEXE_NODUP', 'SEXE_NODUP', label='SEXE')\n",
    "comparer.exact('zipcode', 'zipcode', label='zipcode')\n",
    "comparer.geo(left_on_lat = 'lat_masked',left_on_lng = 'lon_masked',right_on_lat = 'lat_masked',right_on_lng = 'lon_masked',scale = 0.1, offset = 0.5, method = 'exp',label = 'distance')\n",
    "features = comparer.compute(pairs, lca_mask_link, lamal_mask_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the comparison results.\n",
    "features.sum(axis=1).value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[features.distance.between(0.01,0.999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[features.distance.between(0.5,0.999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = rl.ECMClassifier(binarize=0.5)\n",
    "cl.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters that are trained (m, u and p). Note that the estimates\n",
    "# are very good.\n",
    "print(\"p probability P(Match):\", cl.p)\n",
    "print(\"m probabilities P(x_i=1|Match):\", cl.m_probs)\n",
    "print(\"u probabilities P(x_i=1|Non-Match):\", cl.u_probs)\n",
    "print(\"log m probabilities P(x_i=1|Match):\", cl.log_m_probs)\n",
    "print(\"log u probabilities P(x_i=1|Non-Match):\", cl.log_u_probs)\n",
    "print(\"log weights of features:\", cl.log_weights)\n",
    "print(\"weights of features:\", cl.weights)\n",
    "\n",
    "# evaluate the model\n",
    "links_pred = cl.predict(features)\n",
    "print(\"Predicted number of links:\", len(links_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the match probability for each pair in the dataset.\n",
    "probs = cl.prob(features)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv(data_folder/'Clean_data'/'features_w_zipcode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.to_csv(data_folder/'Clean_data'/'probs_w_zipcode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs = pd.DataFrame(probs).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs.columns = ['id_lca','id_lamal','prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id_index_lamal = lamal_mask_link['ID_LAMAL'].astype(int).to_dict()\n",
    "key_id_index_lca = lca_mask_link['ID_LCA'].astype(int).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs['id_lca'] = df_probs['id_lca'].astype(int).map(key_id_index_lca)\n",
    "df_probs['id_lamal'] = df_probs['id_lamal'].astype(int).map(key_id_index_lamal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs['id'] = df_probs['id_lca'].astype(str) + '-'+ df_probs['id_lamal'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_probs = df_probs.set_index('id')['prob'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs = df_probs.set_index('id_lamal').groupby(['id_lca'])['prob'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs_df = pd.DataFrame(max_probs).reset_index()\n",
    "max_probs_df.columns = ['id_lca','id_lamal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs_df['id'] = max_probs_df['id_lca'].astype(str) + '-'+ max_probs_df['id_lamal'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs_df['prob'] = max_probs_df['id'].map(key_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs_df[['id_lca']].to_csv(data_folder/'Clean_data'/'lca_list_pour_christophe.csv')\n",
    "max_probs_df[['id_lamal']].to_csv(data_folder/'Clean_data'/'lamal_list_pour_christophe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "## Export end file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs_df.to_csv(data_folder/'Clean_data'/'max_probs_w_zipcode.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recordLinkage",
   "language": "python",
   "name": "recordlinkage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
