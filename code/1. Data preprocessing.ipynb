{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/Users/david/Dropbox/PhD/Scripts/Spatial analyses')\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geoplot\n",
    "import geoplot.crs as gcrs\n",
    "import datashader as ds, colorcet as cc\n",
    "# import holoviews as hv\n",
    "# hv.extension(\"bokeh\")\n",
    "# from holoviews.element.tiles import EsriImagery\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "import fiona\n",
    "import seaborn as sns\n",
    "from datashader.utils import export_image\n",
    "# from holoviews.operation.datashader import datashade\n",
    "import matplotlib.pyplot as plt\n",
    "import libpysal as lps\n",
    "from scipy.spatial import cKDTree\n",
    "from libpysal.weights.distance import get_points_array\n",
    "from esda import fdr\n",
    "import contextily as ctx\n",
    "import pyspace\n",
    "from importlib import reload\n",
    "plt.rc('font', family='Helvetica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classy_map_ch(df, col, cmap, title, _reversed = True):\n",
    "    filename = title+'.png'\n",
    "    if not os.path.isfile(result_folder/'Maps features'/filename):\n",
    "        if _reversed:\n",
    "            cmap += '_r'\n",
    "        ax = df.to_crs(21781).plot(col, markersize=0.05, linewidth = 0.1, cmap = cmap, legend = True,figsize = (8, 8), legend_kwds = {'shrink':0.5})\n",
    "        lakes.plot(color = 'lightblue', ax=ax)\n",
    "        cantons_ch.geometry.boundary.plot(ax=ax,edgecolor='k', color=None, linewidth=0.1)\n",
    "        plt.imshow(out_image.squeeze(),extent=ch_extent, cmap='Greys_r', alpha=0.4)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(title, fontsize=15, color= 'grey')\n",
    "        plt.savefig(result_folder/'Maps features'/filename, dpi=600, bbox_inches='tight')\n",
    "        return ax\n",
    "    else:\n",
    "        print('Map already generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(axs, orient=\"v\",digits = 2, fontsize = 8, space=.05):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.02)\n",
    "                value = '{:.{}f}'.format(p.get_height(), digits)\n",
    "                ax.text(_x, _y, value,size = fontsize, ha=\"center\") \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.{}f}'.format(p.get_width(), digits)\n",
    "                ax.text(_x, _y, value,size = fontsize, ha=\"left\")\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax, digits)\n",
    "    else:\n",
    "        _single(axs)\n",
    "def optimize_df(df):\n",
    "    \"\"\"\n",
    "    Convert each column of a pandas DataFrame to the datatype that takes the lowest memory.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The converted DataFrame with lowest memory datatypes for each column.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, convert all object columns to category type\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[obj_cols] = df[obj_cols].astype('category')\n",
    "\n",
    "    # Next, loop through all numeric columns and downcast the data types\n",
    "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            # Use smallest integer type possible\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "#         else:\n",
    "#             # Use smallest float type possible ! Bug 'halffloat' not supported by Arrow ! -> Commenting out\n",
    "#             c_min = df[col].min()\n",
    "#             c_max = df[col].max()\n",
    "#             if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "#                 df[col] = df[col].astype('float32')\n",
    "#             else:\n",
    "#                 df[col] = df[col].astype('float64')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder  = Path('../data/')\n",
    "result_folder = Path('../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insurance claims\n",
    "\n",
    "# Linkage\n",
    "df_paires_lamal_lca = pd.read_csv(data_folder/'max_probs_w_zipcode_pour_david_w_uuid.csv')\n",
    "\n",
    "dict_lamal_to_uuid = df_paires_lamal_lca.set_index('id_lamal')['uuid'].to_dict()\n",
    "dict_lca_to_uuid = df_paires_lamal_lca.set_index('id_lca')['uuid'].to_dict()\n",
    "\n",
    "dict_lamal_to_lca = df_paires_lamal_lca.set_index('id_lamal')['id_lca'].to_dict()\n",
    "dict_lca_to_lamal = df_paires_lamal_lca.set_index('id_lca')['id_lamal'].to_dict()\n",
    "\n",
    "df_aos_address = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_aos_address.parquet.gzip'))\n",
    "df_multiple_address_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_aos.parquet.gzip'))\n",
    "df_multiple_address_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_lca.parquet.gzip'))\n",
    "df_remaining_multiple_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_remaining_multiple_lca.parquet.gzip'))\n",
    "##\n",
    "df_couverture_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_couverture_aos.parquet.gzip'))\n",
    "df_flag_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_flag_aos.parquet.gzip'))\n",
    "df_drug_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_drug_aos.parquet.gzip'))\n",
    "df_couverture_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_couverture_lca.parquet.gzip'))\n",
    "df_prestation_aos = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_prestation_aos.parquet.gzip'))\n",
    "df_prestation_lca = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_prestation_lca.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prestation_aos_cam = pd.read_csv(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'Santeintegra_TARMED_080523.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between TARMED codes and their associated category\n",
    "dict_tarmed_cam = {'0.1710':'Acupuncture',\n",
    "                   '0.1720':'Acupuncture',\n",
    "                   '0.1730':'Acupuncture',\n",
    "                   '0.1735':'Acupuncture',\n",
    "                   '0.1740':'Neural therapy',\n",
    "                   '0.1750':'Neural therapy',\n",
    "                   '0.1760':'Neural therapy',\n",
    "                   '0.1770':'Homeopathy',\n",
    "                   '0.1780':'Homeopathy',\n",
    "                   '0.1790':'Homeopathy',\n",
    "                   '0.1800':'Homeopathy',\n",
    "                   '0.1800':'Homeopathy',\n",
    "                   '0.1810':'Traditional Chinese medicine',\n",
    "                   '0.1820':'Traditional Chinese medicine',\n",
    "                   '0.1830':'Traditional Chinese medicine',\n",
    "                   '0.1840':'Anthroposophic medicine',\n",
    "                   '0.1850':'Anthroposophic medicine',\n",
    "                   '0.1860':'Anthroposophic medicine',\n",
    "                   '0.1870':'Phytotherapy',\n",
    "                   '0.1871':'Phytotherapy',\n",
    "                   '0.1872':'Phytotherapy',\n",
    "                   '0.1880':'Phone consultation',\n",
    "                   '0.1890':'Phone consultation',\n",
    "                   '0.1895':'Phone consultation',\n",
    "                   '0.1896':'Phone consultation',\n",
    "                   '0.1900':'Phone consultation',\n",
    "\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prestation_aos_cam['CDPOSITION_categories'] = df_prestation_aos_cam['CDPOSITION'].map(df_prestation_aos_cam['CDPOSITION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a UUID so that we have a single unique ID instead of pairs of ID_LAMAL-ID_LCA\n",
    "df_prestation_lca['uuid'] = df_prestation_lca['ID_LCA'].map(dict_lca_to_uuid)\n",
    "df_prestation_aos['uuid'] = df_prestation_aos['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "df_prestation_aos_cam['uuid'] = df_prestation_aos_cam['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "\n",
    "df_couverture_lca['uuid'] = df_couverture_lca['ID_LCA'].map(dict_lca_to_uuid)\n",
    "df_couverture_aos['uuid'] = df_couverture_aos['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "\n",
    "df_drug_aos['uuid'] = df_drug_aos['ID_LAMAL'].map(dict_lamal_to_uuid)\n",
    "df_flag_aos['uuid'] = df_flag_aos['ID_LAMAL'].map(dict_lamal_to_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GeoDataFrames\n",
    "df_aos_address = gpd.GeoDataFrame(df_aos_address, crs = 4326, geometry = gpd.points_from_xy(df_aos_address['lon_masked'], df_aos_address['lat_masked']))\n",
    "df_multiple_address_aos = gpd.GeoDataFrame(df_multiple_address_aos, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_aos['lon_masked'], df_multiple_address_aos['lat_masked']))\n",
    "df_multiple_address_lca = gpd.GeoDataFrame(df_multiple_address_lca, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_lca['lon_masked'], df_multiple_address_lca['lat_masked']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date strings to datetime objects\n",
    "df_multiple_address_aos['date'] = pd.to_datetime(df_multiple_address_aos['MIN_of_Date_adress'], format='%d%b%y')\n",
    "# Extract the month and year as separate columns\n",
    "df_multiple_address_aos['month'] = df_multiple_address_aos['date'].dt.month\n",
    "df_multiple_address_aos['year'] = df_multiple_address_aos['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date strings to datetime objects\n",
    "df_multiple_address_lca['date'] = pd.to_datetime(df_multiple_address_lca['MIN_of_Date_adress'], format='%d%b%y')\n",
    "# Extract the month and year as separate columns\n",
    "df_multiple_address_lca['month'] = df_multiple_address_lca['date'].dt.month\n",
    "df_multiple_address_lca['year'] = df_multiple_address_lca['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(df, year_col='ANNEE_TRAITEMENT', month_col='MOIS_TRAITEMENT'):\n",
    "    df['treatmentdate'] = pd.to_datetime(df[year_col].astype('string') + '-' + df[month_col].astype('string'))\n",
    "    df['treatmentmonth'] = df['treatmentdate'].dt.strftime('%Y-%m')\n",
    "    df['treatment_Q'] = df['treatmentdate'].dt.to_period(\"Q\")\n",
    "    return df\n",
    "\n",
    "df_list = [df_prestation_aos, df_prestation_lca, df_prestation_aos_cam, df_drug_aos]\n",
    "for df in df_list:\n",
    "    df = process_date(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dfs\n",
    "df_prestation_aos = optimize_df(df_prestation_aos)\n",
    "df_prestation_lca = optimize_df(df_prestation_lca)\n",
    "df_prestation_aos_cam = optimize_df(df_prestation_aos_cam)\n",
    "df_drug_aos = optimize_df(df_drug_aos)\n",
    "df_couverture_aos = optimize_df(df_couverture_aos)\n",
    "df_couverture_lca = optimize_df(df_couverture_lca)\n",
    "df_flag_aos = optimize_df(df_flag_aos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ID not present in the respective datasets\n",
    "df_prestation_aos['ID_LCA'] = df_prestation_aos['ID_LAMAL'].map(dict_lamal_to_lca)\n",
    "df_prestation_lca['ID_LAMAL'] = df_prestation_lca['ID_LCA'].map(dict_lca_to_lamal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cantons = {'AG':'Aargau',\n",
    "                'AI':'Appenzell Innerrhoden',\n",
    "                'AR':\"Appenzell Ausserrhoden\",\n",
    "                'BE':'Bern',\n",
    "                'BL':'Basel-Landschaft',\n",
    "                'BS':'Basel-Stadt',\n",
    "                'FR':'Fribourg',\n",
    "                'GE':'Genève',\n",
    "                'GL':'Glarus',\n",
    "                'GR':'Graubünden',\n",
    "                'JU':\"Jura\",\n",
    "                'LU':'Luzern',\n",
    "                'NE':'Neuchâtel',\n",
    "                'NW':'Nidwalden',\n",
    "                'OW':'Obwalden',\n",
    "                'SH':'Schaffhausen',\n",
    "                'SZ':'Schwyz',\n",
    "                'SO':'Solothurn',\n",
    "                'SG':'St. Gallen',\n",
    "                'TG':'Thurgau',\n",
    "                'TI':'Ticino',\n",
    "                'UR':'Uri',\n",
    "                'VS':'Valais',\n",
    "                'VD':'Vaud',\n",
    "                'ZG':'Zug',\n",
    "                'ZH':'Zürich'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Update : New address data sent by Christophe on January 27th, 2023    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_aos_address_updated.parquet.gzip'))\n",
    "df_multiple_address_aos_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_aos_updated.parquet.gzip'))\n",
    "df_multiple_address_lca_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_multiple_address_lca_updated.parquet.gzip'))\n",
    "df_remaining_multiple_lca_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_remaining_multiple_lca_updated.parquet.gzip'))\n",
    "df_remaining_multiple_aos_updated = optimize_df(pd.read_parquet(data_folder/'raw'/'GM'/'Full'/'Compressed files'/'df_remaining_multiple_aos_updated.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_address_aos_updated['date'] = pd.to_datetime(df_multiple_address_aos_updated['MIN_of_Date_adress'], format='%d%b%y')\n",
    "df_multiple_address_lca_updated['date'] = pd.to_datetime(df_multiple_address_lca_updated['MIN_of_Date_adress'], format='%d%b%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GeoDataFrames\n",
    "df_aos_address_updated = gpd.GeoDataFrame(df_aos_address_updated, crs = 4326, geometry = gpd.points_from_xy(df_aos_address_updated['lon_masked'], df_aos_address_updated['lat_masked'])).to_crs(2056)\n",
    "df_multiple_address_aos_updated = gpd.GeoDataFrame(df_multiple_address_aos_updated, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_aos_updated['lon_masked'], df_multiple_address_aos_updated['lat_masked'])).to_crs(2056)\n",
    "df_multiple_address_lca_updated = gpd.GeoDataFrame(df_multiple_address_lca_updated, crs = 4326, geometry = gpd.points_from_xy(df_multiple_address_lca_updated['lon_masked'], df_multiple_address_lca_updated['lat_masked'])).to_crs(2056)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Unique address AOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous code now deleted, we saw that there is one ID_LAMAL for which there is two address_id\n",
    "df_aos_address_updated[df_aos_address_updated.duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_address_updated[df_aos_address_updated.duplicated(subset = ['ID_LAMAL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the duplicate, the one we choose is unimportant\n",
    "df_aos_address_updated = df_aos_address_updated.drop_duplicates(['ID_LAMAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Multiple address AOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_multi_aos = df_multiple_address_aos_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply, if any row has the same ID_LAMAL, year, address_id, lon and lat...then it's just a duplicated row, not multiple addresses\n",
    "df_test_multi_aos_nouselessdupli = df_test_multi_aos.sort_values(['NOANNEE','MIN_of_Date_adress']).drop_duplicates(subset = ['ID_LAMAL','address_id','lon_masked','lat_masked'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_multi_aos_nouselessdupli.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check we still have the same number of ID_LAMAL\n",
    "df_test_multi_aos_nouselessdupli.ID_LAMAL.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "All good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ID_LAMAL having multiple address_id\n",
    "dupli_aos = df_test_multi_aos_nouselessdupli[df_test_multi_aos_nouselessdupli.ID_LAMAL.duplicated()].ID_LAMAL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually, there are plenty of these that don't have multiple addresses (great data processing on GM's part...)\n",
    "# This should actually be together with df_aos_address\n",
    "df_test_multi_notmultipleaddress = df_test_multi_aos_nouselessdupli[df_test_multi_aos_nouselessdupli.ID_LAMAL.isin(dupli_aos) == False].sort_values(['ID_LAMAL','NOANNEE'])\n",
    "new_df_multiple_address_aos = pd.concat([df_aos_address_updated, df_test_multi_notmultipleaddress[['ID_LAMAL','address_id','lon_masked','lat_masked']]])\n",
    "new_df_multiple_address_aos['NOANNEE'] = 2017\n",
    "new_df_multiple_address_aos['MIN_of_Date_adress'] = '10JAN17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the ones that actually have real multiple addresses\n",
    "df_test_multi_aos_realmultiaddress = df_test_multi_aos_nouselessdupli[df_test_multi_aos_nouselessdupli.ID_LAMAL.isin(dupli_aos)].sort_values(['ID_LAMAL','NOANNEE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to geomasking we have cases where the coordinates are almost exactly the same, these can be identified by distance calculation and collasped into one\n",
    "same_date_different_address = df_test_multi_aos_realmultiaddress[df_test_multi_aos_realmultiaddress.duplicated(subset = ['ID_LAMAL','date'])].ID_LAMAL.unique()\n",
    "df_same_date_different_address = df_test_multi_aos_realmultiaddress[df_test_multi_aos_realmultiaddress.ID_LAMAL.isin(same_date_different_address)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we have the ones with multiples addresses at different dates aka no problem !\n",
    "df_test_multi_aos_realmultiaddress_noproblem = df_test_multi_aos_realmultiaddress[df_test_multi_aos_realmultiaddress.ID_LAMAL.isin(same_date_different_address) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating distance between addresses of each individual\n",
    "df_same_date_different_address[\"distance\"] = df_same_date_different_address.groupby(\"ID_LAMAL\", observed = True)[\"geometry\"].apply(lambda x: x.distance(x.shift()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the few ones with addresses located at less than or equal to 500m\n",
    "# For these, I could just pick one address per date, assuming that it won't change much since they are very close\n",
    "same_date_different_address_closedistance = df_same_date_different_address[df_same_date_different_address['distance'] <= 500].ID_LAMAL.unique()\n",
    "same_date_different_address_fardistance = df_same_date_different_address[df_same_date_different_address['distance'] > 500].ID_LAMAL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_same_date_different_address_closedistance  = df_same_date_different_address[df_same_date_different_address.ID_LAMAL.isin(same_date_different_address_closedistance)]\n",
    "df_same_date_different_address_fardistance  = df_same_date_different_address[df_same_date_different_address.ID_LAMAL.isin(same_date_different_address_fardistance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only check for a distance threshold some addresses may have one under 500 and another one above, if there is even one under, we don't want it in the far dataset, so we remove it\n",
    "df_same_date_different_address_fardistance = df_same_date_different_address_fardistance[df_same_date_different_address_fardistance.ID_LAMAL.isin(same_date_different_address_closedistance) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking one address per date for close distance duplicates\n",
    "df_same_date_different_address_closedistance_nodupli = df_same_date_different_address_closedistance.drop_duplicates(['MIN_of_Date_adress','ID_LAMAL'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking one address per date for far distance duplicates, we order by ID, date and distance (NaN being last), and keep the last\n",
    "df_same_date_different_address_fardistance_nodupli = df_same_date_different_address_fardistance.sort_values(['ID_LAMAL','date','distance']).drop_duplicates(['ID_LAMAL','date'], keep = 'last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Putting AOS back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Real unique addresses\n",
    "print(df_aos_address_updated.ID_LAMAL.nunique())\n",
    "# 2 \"Fake\" multiple addresses\n",
    "print(df_test_multi_notmultipleaddress.ID_LAMAL.nunique())\n",
    "# 3 Non-problematic multiple addresses\n",
    "print(df_test_multi_aos_realmultiaddress_noproblem.ID_LAMAL.nunique())\n",
    "# 4 Problematic - Same date different addresses - Close distance duplicates\n",
    "print(df_same_date_different_address_closedistance_nodupli.ID_LAMAL.nunique())\n",
    "# 5 Problematic - Same date different addresses - Far distance duplicates\n",
    "print(df_same_date_different_address_fardistance_nodupli.ID_LAMAL.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the different datasets to create a final AOS address df\n",
    "df_aos_addresses_final = pd.concat([df_aos_address_updated,\n",
    "          df_test_multi_notmultipleaddress,\n",
    "          df_test_multi_aos_realmultiaddress_noproblem,\n",
    "          df_same_date_different_address_closedistance_nodupli,\n",
    "          df_same_date_different_address_fardistance_nodupli])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there is any unwanted duplicates\n",
    "print('Number of duplicates: ', df_aos_addresses_final[df_aos_addresses_final.duplicated(subset = ['ID_LAMAL','date'])].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the final number of ID_LAMAL\n",
    "print('Number of ID_LAMAL: ', df_aos_addresses_final.ID_LAMAL.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final['uuid'] = df_aos_addresses_final['ID_LAMAL'].map(dict_lamal_to_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining_multiple_aos_updated.ID_Lamal.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## LCA addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final['ID_LCA'] = df_aos_addresses_final['ID_LAMAL'].map(df_paires_lamal_lca.set_index('id_lamal')['id_lca'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Geographical units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes = gpd.read_file(\"/Users/david/Dropbox/PhD/GitHub/COVID19/input/g2s15.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Country boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_geo = gpd.read_file(\"/Users/david/Dropbox/PhD/GitHub/COVID19/input/g2l15.shp\")\n",
    "with fiona.open(\"/Users/david/Dropbox/PhD/GitHub/COVID19/input/g2l15.shp\", \"r\") as shapefile:\n",
    "    country_geo_fiona = [feature[\"geometry\"] for feature in shapefile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_extent = np.asarray(country_geo.bounds)[0][[0,2,1,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file:\n",
    "relief_raster = rasterio.open('/Users/david/Dropbox/PhD/GitHub/COVID19/input/02-relief-ascii.asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_image, out_transform = rasterio.mask.mask(relief_raster, country_geo_fiona, crop=True, filled=False)\n",
    "out_meta = relief_raster.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch = gpd.read_file(data_folder/'raw/Linkage/swissBOUNDARIES3D_1_3_TLM_KANTONSGEBIET.shp')\n",
    "cantons_ch = cantons_ch.to_crs(21781)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Demography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "statpop = pd.read_csv(data_folder/'raw/OFS/STATPOP/ag-b-00.03-vz2020statpop/STATPOP2020.csv',sep = ';')\n",
    "statpop_ha = statpop.copy()\n",
    "geometry = [Point(xy) for xy in zip(statpop['E_KOORD'], statpop['N_KOORD'])]\n",
    "statpop_point = gpd.GeoDataFrame(statpop, crs=2056, geometry=geometry)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(statpop_ha.E_KOORD, statpop_ha.N_KOORD)]\n",
    "statpop_ha = gpd.GeoDataFrame(statpop_ha, crs=2056, geometry=geometry)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lst = pd.read_pickle('../data/processed/gdf_lst.pkl')\n",
    "gdf_lst = gpd.GeoDataFrame(gdf_lst, geometry = gdf_lst['geometry'])\n",
    "gdf_ndvi = pd.read_pickle('../data/processed/gdf_ndvi.pkl')\n",
    "gdf_ndvi = gpd.GeoDataFrame(gdf_ndvi, geometry = gdf_ndvi['geometry'])\n",
    "pm10 = pd.read_pickle('../data/processed/pm10_2020.pkl')\n",
    "# pm10 = gpd.GeoDataFrame(pm10, geometry = pm10['geometry'])\n",
    "pm25 = pd.read_pickle('../data/processed/pm25_2020.pkl')\n",
    "# pm25 = gpd.GeoDataFrame(pm25, geometry = pm25['geometry'])\n",
    "no2 = pd.read_pickle('../data/processed/no2_2020.pkl')\n",
    "# no2_2020 = gpd.GeoDataFrame(no2_2020, geometry = no2_2020['geometry'])\n",
    "ns_car_day = pd.read_pickle('../data/processed/ns_car_day.pkl')\n",
    "# ns_car_day = gpd.GeoDataFrame(ns_car_day, geometry = ns_car_day['geometry'])\n",
    "ns_car_night = pd.read_pickle('../data/processed/ns_car_night.pkl')\n",
    "# ns_car_night = gpd.GeoDataFrame(ns_car_night, geometry = ns_car_night['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10 = pd.DataFrame(pm10).rename(columns = {'mean':'mean_pm10','median':'median_pm10'})\n",
    "pm25 = pd.DataFrame(pm25).rename(columns = {'mean':'mean_pm25','median':'median_pm25'})\n",
    "no2 = pd.DataFrame(no2).rename(columns = {'mean':'mean_no2','median':'median_no2'})\n",
    "ns_car_day = pd.DataFrame(ns_car_day).rename(columns = {'mean':'mean_carday','median':'median_carday'})\n",
    "ns_car_night = pd.DataFrame(ns_car_night).rename(columns = {'mean':'mean_carnight','median':'median_carnight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_df = pd.concat([pm10, pm25, no2, ns_car_day, ns_car_night], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_replacement(column):\n",
    "    conds = [column > np.percentile(column, 99.9)]\n",
    "    choices = [np.percentile(column, 50)]\n",
    "    return np.select(conds,choices,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_df = pollution_df.apply(lambda x: pct_replacement(x))\n",
    "pollution_df = pd.concat([statpop_ha[['RELI','geometry']], pollution_df], axis = 1)\n",
    "pollution_df = gpd.GeoDataFrame(pollution_df, crs = 2056, geometry = pollution_df['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting polygon geometries to point using centroids of polygons for NDVI\n",
    "gdf_ndvi['geometry_pt'] = gdf_ndvi['geometry'].centroid\n",
    "gdf_ndvi = gdf_ndvi.set_geometry(\"geometry_pt\")\n",
    "\n",
    "# Converting polygon geometries to point using centroids of polygons for LST\n",
    "gdf_lst['geometry_pt'] = gdf_lst['geometry'].centroid\n",
    "gdf_lst = gdf_lst.set_geometry(\"geometry_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(gdf_ndvi, 'mean_ndvi','RdYlGn', 'Indice de végétation', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(gdf_lst, 'mean_lst','magma', 'Température de surface (ºC)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting polygon geometries to point using centroids of polygons for pollution variables\n",
    "pollution_df['geometry_pt'] = pollution_df['geometry'].centroid\n",
    "pollution_df = pollution_df.set_geometry(\"geometry_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_pm10','magma', 'Pollution atmosphérique (PM10)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_pm25','magma', 'Pollution atmosphérique (PM25)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_no2','magma', 'Pollution atmosphérique (NO2)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(pollution_df, 'mean_carday','magma', 'Pollution sonore (dB)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_sep1 = optimize_df(gpd.read_file(data_folder/'raw'/'Swiss-SEP'/'SNC_Swiss-SEP1'/'SHP'/'ssep_user_geo.shp', driver = 'Shapefile'))\n",
    "df_swiss_sep2 = optimize_df(gpd.read_file(data_folder/'raw'/'Swiss-SEP'/'SNC_Swiss-SEP2'/'SHP'/'ssep2_user_geo.shp', driver = 'Shapefile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_2023 = optimize_df(pd.read_csv(data_folder/'raw'/'Swiss-SEP'/'SNC_Swiss-SEP-2023'/'ssep_open.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_2023 = gpd.GeoDataFrame(df_swiss_2023, crs = 2056, geometry=gpd.points_from_xy(df_swiss_2023.geox, df_swiss_2023.geoy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch = cantons_ch.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch[cantons_ch.NAME == 'Genève']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_sep_ge = df_swiss_2023[df_swiss_2023.within(cantons_ch.loc[20].geometry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swiss_sep_ge[['ssep3','geometry']].explore('ssep3', cmap = 'RdYlGn', marker_kwds = {'radius':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_swiss_sep_ge.plot('ssep3', cmap = 'RdYlGn', markersize = 0.5, figsize = (15,15), linewidth=0, legend = True, legend_kwds={'shrink':0.5})\n",
    "ax.set_axis_off()\n",
    "plt.savefig(result_folder/'Maps features'/'Indice de statut socioéconomique - GE.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_swiss_sep_ge.plot('ssep3_d', cmap = 'RdYlGn', vmin = 0, vmax = 10, markersize = 0.5, figsize = (15,15), linewidth=0, legend = True, legend_kwds={'shrink':0.5})\n",
    "ax.set_axis_off()\n",
    "plt.savefig(result_folder/'Maps features'/'Indice de statut socioéconomique (Déciles) - GE.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_swiss_sep2, 'ssep2_d','RdYlGn', 'Indice de statut socioéconomique (Déciles)', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_swiss_sep2, 'ssep2','RdYlGn', 'Indice de statut socioéconomique', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_swiss_2023, 'ssep3_d','RdYlGn', 'Indice de statut socioéconomique 2023 (Déciles)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "## Accessibilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_access_ofs = pd.read_csv(data_folder/'raw/OFS/Accessibility/ag-b-00.03-2018spop-csv.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_access_ofs = gpd.GeoDataFrame(df_access_ofs, crs = 2056, geometry = gpd.points_from_xy(df_access_ofs.E_COORD, df_access_ofs.N_COORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_access_ofs, 'D_MEDIC', 'RdYlGn', 'Accès aux cabinets médicaux et centres ambulatoires')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_access_ofs, 'D_MEDIC_B', 'RdYlGn', 'Accès à la médecine de premier recours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "classy_map_ch(df_access_ofs, 'D_MEDIC_S', 'RdYlGn', 'Accès à la médecine spécialisée')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "### Add sociodemographic and environmental features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Accessibility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(df_access_ofs.E_COORD, df_access_ofs.N_COORD)]\n",
    "\n",
    "df_access_ofs = df_access_ofs.set_geometry(geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access = gpd.sjoin_nearest(df_aos_addresses_final, df_access_ofs.drop(['RELI','E_COORD','N_COORD','YEAR','POP_TOTAL'], axis = 1), how = 'left', distance_col = 'distance_join_access').drop('index_right', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Pollution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(pollution_df.geometry_pt.x, pollution_df.geometry_pt.y)]\n",
    "pollution_df = pollution_df.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution = gpd.sjoin_nearest(df_addresses_w_access, pollution_df.drop(['RELI','geometry_pt'], axis = 1), how = 'left', distance_col = 'distance_join_pollution').drop('index_right', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "NDVI & LST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ndvi_lst = pd.concat([gdf_ndvi.drop(['geometry_pt','geometry'], axis = 1), gdf_lst], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ndvi_lst = gdf_ndvi_lst.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst = gpd.sjoin_nearest(df_addresses_w_access_pollution, gdf_ndvi_lst.drop(['geometry_pt'], axis = 1), how = 'left', distance_col = 'distance_join_ndvi_lst').drop('index_right', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "Socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Polygon(zip([xy[0],xy[0],xy[0]+100,xy[0]+100],[xy[1],xy[1]+100,xy[1]+100,xy[1]])) for xy in zip(df_swiss_2023.geox, df_swiss_2023.geoy)]\n",
    "df_swiss_2023 = df_swiss_2023.set_geometry(geometry)\n",
    "df_swiss_2023.crs = 2056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final_sep = gpd.sjoin_nearest(df_aos_addresses_final, df_swiss_2023, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aos_addresses_final_sep = df_aos_addresses_final_sep.drop_duplicates(subset = ['ID_LAMAL','address_id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst_sep = pd.merge(df_addresses_w_access_pollution_ndvi_lst,df_aos_addresses_final_sep[['uuid','address_id','date','ssep2','ssep2_d','ssep2_t','ssep2_q','ssep3','ssep3_d','ssep3_t','ssep3_q']], on = ['uuid','address_id','date'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst_sep.to_parquet(data_folder/'/processed/df_addresses_with_socio_env.parquet.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addresses_w_access_pollution_ndvi_lst_sep = gpd.GeoDataFrame(df_addresses_w_access_pollution_ndvi_lst_sep, crs = 4326, geometry=gpd.points_from_xy(df_addresses_w_access_pollution_ndvi_lst_sep.lon_masked, df_addresses_w_access_pollution_ndvi_lst_sep.lat_masked))\n",
    "df_addresses_w_access_pollution_ndvi_lst_sep_uniques = df_addresses_w_access_pollution_ndvi_lst_sep[df_addresses_w_access_pollution_ndvi_lst_sep.doubl.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of years\n",
    "years = [2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "# repeat rows for each year\n",
    "df_addresses_w_access_pollution_ndvi_lst_sep_uniques = df_addresses_w_access_pollution_ndvi_lst_sep_uniques.reindex(df_addresses_w_access_pollution_ndvi_lst_sep_uniques.index.repeat(len(years)))\n",
    "df_addresses_w_access_pollution_ndvi_lst_sep_uniques['NOANNEE'] = years * (len(df_addresses_w_access_pollution_ndvi_lst_sep_uniques.index)//len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_address = pd.concat([df_addresses_w_access_pollution_ndvi_lst_sep_uniques, df_addresses_w_access_pollution_ndvi_lst_sep[df_addresses_w_access_pollution_ndvi_lst_sep.doubl.isnull() == False]]).reset_index(drop=True)\n",
    "df_full_address = df_full_address.sort_values(['uuid','date']).drop_duplicates(subset = ['uuid','NOANNEE'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_address.to_parquet(data_folder/'processed/df_full_address.parquet.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_franchise = df_couverture_aos[['ID_LAMAL','MTFRANCHISECOUV']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_franchise = pd.merge(df_franchise, df_address_full[['ID_LAMAL','geometry']].drop_duplicates(), on = 'ID_LAMAL')\n",
    "df_franchise = df_franchise.set_geometry('geometry')\n",
    "df_franchise = df_franchise.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnn16 = lps.weights.KNN(cKDTree(get_points_array(df_franchise.geometry)),50)\n",
    "# getiswnn16 = pyspace.compute_getis(df_franchise,'MTFRANCHISECOUV',wnn16, 999, p_001 = False)\n",
    "# fdr_pvalue = fdr(getiswnn16.p_sim, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df_couverture_aos[['ID_LAMAL','NBAGE']].groupby('ID_LAMAL', observed = True)['NBAGE'].max()\n",
    "df_age = pd.merge(df_age, df_address_full[['ID_LAMAL','geometry']].drop_duplicates(), on = 'ID_LAMAL')\n",
    "df_age = df_age.set_geometry('geometry')\n",
    "df_age = df_age.to_crs(2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df_age.sample(120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age['E'] = df_age['geometry'].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wnn16 = lps.weights.KNN(cKDTree(get_points_array(df_age.geometry)),50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from esda.getisord import G_Local\n",
    "\n",
    "# g = G_Local(df_age['NBAGE'], wnn16, star = True, permutations = 999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getiswnn16 = pyspace.compute_getis(df_age,'NBAGE',wnn16, 999, p_001 = False)\n",
    "# fdr_pvalue = fdr(getiswnn16.p_sim, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_age['NBAGE_G_cl_fdr'] = df_age['NBAGE_G_cl']  \n",
    "# df_age.loc[df_age['NBAGE_G_psim'] >= fdr_pvalue, 'NBAGE_G_cl_fdr'] = 'Not significant'\n",
    "# fig, ax = pyspace.plotGetisMap(df_age,'NBAGE_G_cl',p_001 = False, commune_name = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
