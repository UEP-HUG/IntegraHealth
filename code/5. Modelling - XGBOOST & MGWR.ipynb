{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/Users/david/Dropbox/PhD/Scripts/Spatial analyses')\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geoplot\n",
    "import geoplot.crs as gcrs\n",
    "import datashader as ds, colorcet as cc\n",
    "# import holoviews as hv\n",
    "# hv.extension(\"bokeh\")\n",
    "# from holoviews.element.tiles import EsriImagery\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "import fiona\n",
    "import seaborn as sns\n",
    "from datashader.utils import export_image\n",
    "# from holoviews.operation.datashader import datashade\n",
    "import matplotlib.pyplot as plt\n",
    "import libpysal as lps\n",
    "from scipy.spatial import cKDTree\n",
    "from libpysal.weights.distance import get_points_array\n",
    "from esda import fdr\n",
    "import pyspace\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from importlib import reload\n",
    "plt.rc('font', family='Helvetica')\n",
    "from tableone import TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_folder = Path('../Results/')\n",
    "model_folder = res_folder/'Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Determinants of using an integrative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet('../Data/processed/full_dataset_nonull.parquet.gzip')\n",
    "# data_2017_psm = pd.read_parquet('../Data/processed/data_2017_scaled_and_matched.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = gpd.GeoDataFrame(data, crs = 4326, geometry=gpd.points_from_xy(data.lon_masked, data.lat_masked))\n",
    "\n",
    "# data = data.to_crs(2056)\n",
    "# data['E'], data['N'] = data['geometry'].x, data['geometry'].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_final['gp'] = data_final['gp'].astype(str)\n",
    "# data_final = data_final.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated_filtered = pd.read_parquet('../Data/processed/df_treated_filtered.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated_filtered.groupby('NOANNEE').size().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated_filtered['MTFRANCHISECOUV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_final = data[data.treatment.isnull()==False]\n",
    "\n",
    "df_treated_filtered['DEDUCTIBLE_above_500'] = 1\n",
    "df_treated_filtered.loc[df_treated_filtered.MTFRANCHISECOUV < 500, 'DEDUCTIBLE_above_500'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Add chronic diseases info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_final[data_final['Diabetes mellitus'] > 0].uuid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reverse the dictionary for grouping by disease\n",
    "# disease_to_atc = {}\n",
    "# for atc, disease in dict_chronic_diseases_atc_amount.items():\n",
    "#     if disease in disease_to_atc:\n",
    "#         disease_to_atc[disease].append(atc)\n",
    "#     else:\n",
    "#         disease_to_atc[disease] = [atc]\n",
    "\n",
    "# # For each disease, create a new column and check if any ATC columns are 1\n",
    "# for disease, atcs in disease_to_atc.items():\n",
    "#     data_final[disease] = data_final[atcs].sum(axis=1)\n",
    "\n",
    "# # If you want binary 0/1 values\n",
    "# # data_final = data_final.replace({disease: {2: 1 for disease in disease_to_atc.keys()}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 1. Baseline model - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_treated_filtered[['NBAGE_std','ssep3_std','cds_std','E_std','N_std','D_MEDIC_B_std','D_MEDIC_S_std']] = scaler.fit_transform(df_treated_filtered[['NBAGE','ssep3','cds','E','N','D_MEDIC_B','D_MEDIC_S']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_final['CANTON'] = data_final.filter(regex='SEX_').idxmax(axis=1).str.replace('CANTON_NAME_', '')\n",
    "# data_final['SEX'] = data_final.filter(regex='SEX_').idxmax(axis=1).str.replace('SEX_', '')\n",
    "# data_final['LANG'] = data_final.filter(regex='LANG_').idxmax(axis=1).str.replace('LANG_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_treated_filtered['NBRE_FACTURES_AOS'] = df_treated_filtered['NBRE_FACTURES_AOS'].astype(float)\n",
    "df_treated_filtered['n_atc'] = df_treated_filtered['n_atc'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_2017 = data_final[data_final.NOANNEE == 2017]\n",
    "# data_2018 = data_final[data_final.NOANNEE == 2018]\n",
    "# data_2019 = data_final[data_final.NOANNEE == 2019]\n",
    "# data_2020 = data_final[data_final.NOANNEE == 2020]\n",
    "# data_2021 = data_final[data_final.NOANNEE == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_chronic_diseases_atc = {\n",
    "    \"A02A_ATC_N\":'Acid related disorders',\n",
    "    \"A02B_ATC_N\":'Acid related disorders',\n",
    "    'M05B_ATC_N':'Osteoporosis',\n",
    "    'A10A_ATC_N':'Diabetes mellitus',\n",
    "    'A10B_ATC_N':'Diabetes mellitus',\n",
    "    'B03A_ATC_N':'Iron deficiency anemia',\n",
    "    'N03A_ATC_N': 'Epilepsy',\n",
    "    'C02A_ATC_N':'Cardiovascular diseases',\n",
    "    'C02C_ATC_N':'Cardiovascular diseases',\n",
    "    'C02D_ATC_N':'Cardiovascular diseases',\n",
    "    'C02K_ATC_N':'Cardiovascular diseases',\n",
    "    'C04A_ATC_N':'Cardiovascular diseases',\n",
    "    'C07A_ATC_N':'Cardiovascular diseases',\n",
    "    'C08C_ATC_N':'Cardiovascular diseases',\n",
    "    'C08D_ATC_N':'Cardiovascular diseases',\n",
    "    'C09A_ATC_N':'Cardiovascular diseases',\n",
    "    'C09B_ATC_N':'Cardiovascular diseases',\n",
    "    'C09C_ATC_N':'Cardiovascular diseases',\n",
    "    'C09D_ATC_N':'Cardiovascular diseases',\n",
    "    \"C10A_ATC_N\":\"Hyperlipidemia\",\n",
    "    \"C10B_ATC_N\":\"Hyperlipidemia\",\n",
    "    'L01A_ATC_N':'Cancer',\n",
    "    'L01B_ATC_N':'Cancer',\n",
    "    'L01C_ATC_N':'Cancer',\n",
    "    'L01D_ATC_N':'Cancer',\n",
    "    'L01E_ATC_N':'Cancer',\n",
    "    'L01F_ATC_N':'Cancer',\n",
    "    'L01X_ATC_N':'Cancer',\n",
    "    \"M01A_ATC_N\":'Rheumatologic conditions',\n",
    "    \"M01C_ATC_N\":'Rheumatologic conditions',\n",
    "    \"M02A_ATC_N\":'Rheumatologic conditions',\n",
    "    \"M04A_ATC_N\":'Gout and hyperuricemia',\n",
    "    'N02A_ATC_N':'Pain',\n",
    "    'N02B_ATC_N':\"Pain\",\n",
    "    \"N02C_ATC_N\":'Migraines',\n",
    "    \"N04A_ATC_N\":\"Parkinson's disease\",\n",
    "    \"N04B_ATC_N\":\"Parkinson's disease\",\n",
    "    'N05A_ATC_N':'Psychoses',\n",
    "\n",
    "    'N05B_ATC_N':'Psychological disorders',\n",
    "    'N05C_ATC_N':'Psychological disorders',\n",
    "    'N06A_ATC_N':'Psychological disorders',\n",
    "\n",
    "    \"N06D_ATC_N\":'Dementia',\n",
    "    \"L04A_ATC_N\":'Rheumatologic conditions',\n",
    "    \"H03A_ATC_N\":'Thyroid disorders',\n",
    "    \"J04A_ATC_N\":'Tuberculosis',\n",
    "    'J05A_ATC_N':'HIV',\n",
    "    \"R03A_ATC_N\":'Respiratory illness',\n",
    "    \"S01E_ATC_N\":'Glaucoma'\n",
    "}\n",
    "\n",
    "dict_chronic_diseases_atc_amount = {\n",
    "    \"A02A_ATC_AMOUNT\":'Acid related disorders',\n",
    "    \"A02B_ATC_AMOUNT\":'Acid related disorders',\n",
    "    'M05B_ATC_AMOUNT':'Osteoporosis',\n",
    "    'A10A_ATC_AMOUNT':'Diabetes mellitus',\n",
    "    'A10B_ATC_AMOUNT':'Diabetes mellitus',\n",
    "    'B03A_ATC_AMOUNT':'Iron deficiency anemia',\n",
    "    'N03A_ATC_AMOUNT': 'Epilepsy',\n",
    "    'C02A_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C02C_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C02D_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C02K_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C04A_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C07A_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C08C_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C08D_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C09A_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C09B_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C09C_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    'C09D_ATC_AMOUNT':'Cardiovascular diseases',\n",
    "    \"C10A_ATC_AMOUNT\":\"Hyperlipidemia\",\n",
    "    \"C10B_ATC_AMOUNT\":\"Hyperlipidemia\",\n",
    "    'L01A_ATC_AMOUNT':'Cancer',\n",
    "    'L01B_ATC_AMOUNT':'Cancer',\n",
    "    'L01C_ATC_AMOUNT':'Cancer',\n",
    "    'L01D_ATC_AMOUNT':'Cancer',\n",
    "    'L01E_ATC_AMOUNT':'Cancer',\n",
    "    'L01F_ATC_AMOUNT':'Cancer',\n",
    "    'L01X_ATC_AMOUNT':'Cancer',\n",
    "    \"M01A_ATC_AMOUNT\":'Rheumatologic conditions',\n",
    "    \"M01C_ATC_AMOUNT\":'Rheumatologic conditions',\n",
    "    \"M02A_ATC_AMOUNT\":'Rheumatologic conditions',\n",
    "    \"M04A_ATC_AMOUNT\":'Gout and hyperuricemia',\n",
    "    'N02A_ATC_AMOUNT':'Pain',\n",
    "    'N02B_ATC_AMOUNT':\"Pain\",\n",
    "    \"N02C_ATC_AMOUNT\":'Migraines',\n",
    "    \"N04A_ATC_AMOUNT\":\"Parkinson's disease\",\n",
    "    \"N04B_ATC_AMOUNT\":\"Parkinson's disease\",\n",
    "    'N05A_ATC_AMOUNT':'Psychoses',\n",
    "    'N05B_ATC_AMOUNT':'Psychological disorders',\n",
    "    'N05C_ATC_AMOUNT':'Psychological disorders',\n",
    "    'N06A_ATC_AMOUNT':'Psychological disorders',\n",
    "    \"N06D_ATC_AMOUNT\":'Dementia',\n",
    "    \"L04A_ATC_AMOUNT\":'Rheumatologic conditions',\n",
    "    \"H03A_ATC_AMOUNT\":'Thyroid disorders',\n",
    "    \"J04A_ATC_AMOUNT\":'Tuberculosis',\n",
    "    'J05A_ATC_AMOUNT':'HIV',\n",
    "    \"R03A_ATC_AMOUNT\":'Respiratory illness',\n",
    "\n",
    "    \"S01E_ATC_AMOUNT\":'Glaucoma',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "chronic_diseases = list(set(dict_chronic_diseases_atc.values()))\n",
    "chronic_diseases_binary = [s + '_binary' for s in chronic_diseases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_vars = ['NBAGE_std','SEX_F', 'ssep3_std','cds', 'DEDUCTIBLE_above_500', 'E', 'N']\n",
    "exog = sm.add_constant(df_treated_filtered[exog_vars])\n",
    "endog = df_treated_filtered['treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "' + '.join(df_treated_filtered.filter(regex='region_').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression formula using your covariates\n",
    "formula = \"treatment ~ NBAGE_std + SEX_F + region_DE + region_FR + region_IT + ssep3_std + urb_Urbain + urb_Périurbain + cds_std + D_MEDIC_B_std + D_MEDIC_S_std + DEDUCTIBLE_above_500 + Asthma_PCG + Cancer_PCG + Diabetes_PCG + Epilepsy_PCG + Glaucoma_PCG + HIV_AIDS_PCG + Heart_disease_PCG + Hypertension_related_PCG + Immune_PCG + Inflammatory_PCG + Mental_PCG + Other_PCG + Pain_PCG + Parkinson_PCG + Thyroid_PCG + mean_ndvi + mean_lst + mean_pm10 + mean_pm25 + mean_no2 + mean_carnight + E_std + N_std + E_std*N_std\"\n",
    "\n",
    "# Fit the logistic regression model using statsmodels\n",
    "logit_model = smf.logit(formula, data=df_treated_filtered).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated_filtered.filter(regex='PCG').columns.str.replace('_PCG','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated_filtered.filter(regex='region').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(['NBAGE',\"NBAGE_std\", \"ssep3_std\",'ssep3','region_DE', 'region_FR', 'region_IT','Asthma_PCG', 'Cancer_PCG', 'Diabetes_PCG', 'Epilepsy_PCG',\n",
    "       'Glaucoma_PCG', 'HIV_AIDS_PCG', 'Heart_disease_PCG',\n",
    "       'Hypertension_related_PCG', 'Immune_PCG', 'Inflammatory_PCG',\n",
    "       'Mental_PCG', 'Other_PCG', 'Pain_PCG', 'Parkinson_PCG', 'Thyroid_PCG', \"SEX_F\",'SEX','LANG', \"cds_std\",'cds','LANG_FR','D_MEDIC_B','D_MEDIC_S','D_MEDIC_B_std','D_MEDIC_S_std','DEDUCTIBLE_above_500','E_std','N_std','E_std:N_std','PRESTATIONS_TOTAL','PRESTATIONS_BRUTES_AOS','PRESTATIONS_BRUTES_LCA','AMBULATOIRE','STATIONNAIRE','PRESTATIONS_ACCIDENT','PRESTATIONS_DISEASE','PRESTATIONS_BIRTH','MTFRANCHISECOUV','mean_pm10','mean_no2','mean_pm25','mean_ndvi','mean_lst','mean_carnight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(['Age',\"Age\", \"SES index\",'SES index''Asthma', 'Cancer', 'Diabetes', 'Epilepsy', 'Glaucoma', 'HIV_AIDS',\n",
    "       'Heart_disease', 'Hypertension_related', 'Immune', 'Inflammatory',\n",
    "       'Mental', 'Other', 'Pain', 'Parkinson', 'Thyroid', \"Sex (Female)\",'Sex','Langage', \"CDS\",'CDS','French speaker','Access to prim. care med.','Access to spec. med.','Access to prim. care med.','Access to spec. med.','Franchise (>500)','E','N','E:N','Montant tot. prestations','Montant tot. prestations (AOS)','Montant tot. prestations (LCA)','Montant tot. ambulatoire','Montant tot. stationnaire','Montant tot. accident','Montant tot. maladie','Montant tot. maternité','Franchise','PM10','NO2','PM25','NDVI','LST','Nighttime car noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = pd.DataFrame({\"old\": ['NBAGE',\"NBAGE_std\", \"ssep3_std\",'ssep3', 'region_DE', 'region_FR', 'region_IT','urb_Urbain','urb_Périurbain','Asthma_PCG', 'Cancer_PCG', 'Diabetes_PCG', 'Epilepsy_PCG',\n",
    "       'Glaucoma_PCG', 'HIV_AIDS_PCG', 'Heart_disease_PCG',\n",
    "       'Hypertension_related_PCG', 'Immune_PCG', 'Inflammatory_PCG',\n",
    "       'Mental_PCG', 'Other_PCG', 'Pain_PCG', 'Parkinson_PCG', 'Thyroid_PCG', \"SEX_F\",'SEX','LANG', \"cds_std\",'cds','LANG_FR','D_MEDIC_B','D_MEDIC_S','D_MEDIC_B_std','D_MEDIC_S_std','DEDUCTIBLE_above_500','E_std','N_std','E_std:N_std','PRESTATIONS_TOTAL','PRESTATIONS_BRUTES_AOS','PRESTATIONS_BRUTES_LCA','AMBULATOIRE','STATIONNAIRE','PRESTATIONS_ACCIDENT','PRESTATIONS_DISEASE','PRESTATIONS_BIRTH','MTFRANCHISECOUV','mean_pm10','mean_no2','mean_pm25','mean_ndvi','mean_lst','mean_carnight'],\n",
    "                           \"new\": ['Age',\"Age\", \"SES index\",'SES index','region_DE', 'region_FR', 'region_IT','Urban','Periurban', 'Asthma', 'Cancer', 'Diabetes', 'Epilepsy', 'Glaucoma', 'HIV/AIDS',\n",
    "       'Heart disease', 'Hypertension related', 'Immune', 'Inflammatory',\n",
    "       'Mental', 'Other', 'Pain', 'Parkinson', 'Thyroid', \"Sex (Female)\",'Sex','Langage', \"CDS\",'CDS','French speaker','Access to prim. care med.','Access to spec. med.','Access to prim. care med.','Access to spec. med.','Franchise (>500)','E','N','E:N','Montant tot. prestations','Montant tot. prestations (AOS)','Montant tot. prestations (LCA)','Montant tot. ambulatoire','Montant tot. stationnaire','Montant tot. accident','Montant tot. maladie','Montant tot. maternité','Franchise','PM10','NO2','PM25','NDVI','LST','Nighttime car noise']})\n",
    "def update_variable_names(summary_table, variable_names, table_type):\n",
    "    name_mapper = variable_names.set_index('old')['new'].to_dict()\n",
    "    if table_type == 'summary':\n",
    "        name_mapper = {f\"{key}, mean (SD)\": f\"{value}, mean (SD)\" for key, value in name_mapper.items()}\n",
    "\n",
    "    summary_table = summary_table.rename(index=name_mapper)\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = logit_model.params\n",
    "conf = logit_model.conf_int()\n",
    "conf['Odds Ratio'] = params\n",
    "conf.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
    "# convert log odds to ORs\n",
    "odds = pd.DataFrame(np.exp(conf))\n",
    "# check if pvalues are significant\n",
    "odds['pvalues'] = logit_model.pvalues\n",
    "odds['significant?'] = ['significant' if pval <= 0.05 else 'not significant' for pval in logit_model.pvalues]\n",
    "updated_odds = update_variable_names(odds, variable_names, 'odds')\n",
    "updated_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_plot(df_odds, folder, period):\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    fig, ax = plt.subplots(nrows=1, sharex=True, sharey=True, figsize=(6, 6), dpi=300)\n",
    "    for idx, row in df_odds.iloc[::-1].iterrows():\n",
    "        ci = [[row['Odds Ratio'] - row[::-1]['2.5%']], [row['97.5%'] - row['Odds Ratio']]]\n",
    "        if row['significant?'] == 'significant':\n",
    "            if row['Odds Ratio'] > 1:\n",
    "                plt.errorbar(x=[row['Odds Ratio']], y=[row.name], xerr=ci,\n",
    "                    ecolor='tab:red', capsize=3, linestyle='None', linewidth=1, marker=\"o\", \n",
    "                             markersize=5, mfc=\"tab:red\", mec=\"tab:red\")\n",
    "            else:\n",
    "                plt.errorbar(x=[row['Odds Ratio']], y=[row.name], xerr=ci,\n",
    "                    ecolor='tab:blue', capsize=3, linestyle='None', linewidth=1, marker=\"o\", \n",
    "                             markersize=5, mfc=\"tab:blue\", mec=\"tab:blue\")\n",
    "        else:\n",
    "            plt.errorbar(x=[row['Odds Ratio']], y=[row.name], xerr=ci,\n",
    "                ecolor='tab:gray', capsize=3, linestyle='None', linewidth=1, marker=\"o\", \n",
    "                         markersize=5, mfc=\"tab:gray\", mec=\"tab:gray\")\n",
    "    plt.axvline(x=1, linewidth=0.8, linestyle='--', color='black')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=8)\n",
    "    plt.xlabel('Odds Ratio and 95% Confidence Interval', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(folder/'forest_plot_{}.png'.format(period))\n",
    "    plt.show()\n",
    "model_directory = res_folder /'Determinants of integrative medicine'/ 'Logistic regression - Full period'\n",
    "forest_plot(updated_odds.sort_values('Odds Ratio'), model_directory, 'Full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['PRESTATIONS_TOTAL','PRESTATIONS_BRUTES_AOS','PRESTATIONS_BRUTES_LCA','D_MEDIC_S','D_MEDIC_B','AMBULATOIRE','STATIONNAIRE','PRESTATIONS_ACCIDENT','PRESTATIONS_DISEASE','PRESTATIONS_BIRTH']\n",
    "mytable = TableOne(data_final, columns, groupby='NOANNEE', \n",
    "                   pval=True,\n",
    "                   pval_adjust='bonferroni', \n",
    "                   htest_name=True, \n",
    "                   missing=False)\n",
    "summary_table_year = update_variable_names(mytable.tableone,variable_names,'summary')\n",
    "summary_table_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['PRESTATIONS_TOTAL','PRESTATIONS_BRUTES_AOS','PRESTATIONS_BRUTES_LCA','D_MEDIC_S','D_MEDIC_B','AMBULATOIRE','STATIONNAIRE','PRESTATIONS_ACCIDENT','PRESTATIONS_DISEASE','PRESTATIONS_BIRTH']\n",
    "mytable = TableOne(data_final, columns, groupby='gp', \n",
    "                   pval=True,\n",
    "                   pval_adjust='bonferroni', \n",
    "                   htest_name=True, \n",
    "                   missing=False)\n",
    "summary_table_gp = update_variable_names(mytable.tableone, variable_names, 'summary')\n",
    "summary_table_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['PRESTATIONS_TOTAL','PRESTATIONS_BRUTES_AOS','PRESTATIONS_BRUTES_LCA','treatment','D_MEDIC_S','D_MEDIC_B','AMBULATOIRE','STATIONNAIRE','PRESTATIONS_ACCIDENT','PRESTATIONS_DISEASE','PRESTATIONS_BIRTH']\n",
    "mytable = TableOne(data_final, columns, groupby=['LANG'], \n",
    "                   pval=True,\n",
    "                   pval_adjust='bonferroni', \n",
    "                   htest_name=True, \n",
    "                   missing=False)\n",
    "summary_table_lang = update_variable_names(mytable.tableone, variable_names, 'summary')\n",
    "summary_table_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['SEX','LANG']\n",
    "columns = ['NBAGE','SEX','LANG','MTFRANCHISECOUV','ssep2','cds','PRESTATIONS_TOTAL','PRESTATIONS_BRUTES_AOS','PRESTATIONS_BRUTES_LCA','AMBULATOIRE','STATIONNAIRE','D_MEDIC_S','D_MEDIC_B','PRESTATIONS_ACCIDENT','PRESTATIONS_DISEASE','PRESTATIONS_BIRTH']\n",
    "mytable = TableOne(data_final, columns, categorical, groupby='gp', \n",
    "                   pval=True, \n",
    "                   pval_adjust='bonferroni', \n",
    "                   htest_name=True, \n",
    "                   missing=False, \n",
    "                   row_percent=True,\n",
    "                   tukey_test=True,\n",
    "                   normal_test=True)\n",
    "summary_table_1 = update_variable_names(mytable.tableone, variable_names, 'summary')\n",
    "summary_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['n_chronic_diseases'] = data_final[chronic_diseases_binary].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.loc[data_final['n_chronic_diseases'] > 8,'n_chronic_diseases'] = '9+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['n_chronic_diseases'] = data_final['n_chronic_diseases'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['n_chronic_diseases']\n",
    "columns = ['n_chronic_diseases']\n",
    "mytable = TableOne(data_final[data_final.gp.isin(['AOS only','LCA & AOS'])], columns, categorical, groupby='gp', \n",
    "                   pval=True, \n",
    "                   pval_adjust='bonferroni', \n",
    "                   htest_name=True, \n",
    "                   missing=False, \n",
    "                   row_percent=True,\n",
    "                   tukey_test=True,\n",
    "                   normal_test=True)\n",
    "summary_table_1 = mytable.tableone\n",
    "summary_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = chronic_diseases_binary\n",
    "columns = chronic_diseases_binary\n",
    "mytable = TableOne(data_final[data_final.gp.isin(['AOS only','LCA & AOS'])], columns, categorical, groupby='gp', \n",
    "                   pval=True, \n",
    "                   pval_adjust='bonferroni', \n",
    "                   htest_name=True, \n",
    "                   missing=False, \n",
    "                   row_percent=True,\n",
    "                   tukey_test=True,\n",
    "                   normal_test=True)\n",
    "summary_table_1 = mytable.tableone\n",
    "summary_table_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Chronic diseases vs treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with 'complementary_spend' and 'conventional_spend_for_disease_X'\n",
    "data_final['intercept'] = 1  # Add an intercept term\n",
    "\n",
    "# Specify the multiple linear regression model\n",
    "logit_model = sm.Logit(data_final['treatment'], data_final[chronic_diseases_binary].rename(\n",
    "    columns=lambda x: x.replace(\"_binary\", \"\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(result.summary())\n",
    "\n",
    "params = result.params\n",
    "conf = result.conf_int()\n",
    "conf['Odds Ratio'] = params\n",
    "conf.columns = ['2.5%', '97.5%', 'Odds Ratio']\n",
    "# convert log odds to ORs\n",
    "odds = pd.DataFrame(np.exp(conf))\n",
    "# check if pvalues are significant\n",
    "odds['pvalues'] = result.pvalues\n",
    "odds['significant?'] = ['significant' if pval <= 0.05 else 'not significant' for pval in result.pvalues]\n",
    "\n",
    "model_directory = res_folder /'Determinants of integrative medicine'/ 'Logistic regression - Chronic diseases - Full period'\n",
    "forest_plot(odds.sort_values('Odds Ratio'), model_directory, 'Chronic diseases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Association between amount spent and diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_OLS_coefficients(result, var):\n",
    "    # Get coefficients and confidence intervals\n",
    "    coefficients = result.params\n",
    "    conf_int = result.conf_int()\n",
    "    conf_int['coef'] = coefficients\n",
    "    conf_int['pvalues'] = result.pvalues\n",
    "    conf_int['significant?'] = ['significant' if pval <= 0.05 else 'not significant' for pval in result.pvalues]\n",
    "    \n",
    "    # Sort by coefficients\n",
    "    sorted_coef = conf_int.sort_values(by='coef')\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    for idx, row in sorted_coef.iterrows():\n",
    "        if idx == 'intercept':\n",
    "            pass\n",
    "        else:\n",
    "            ci_lower = row[0]  # Lower bound of confidence interval\n",
    "            ci_upper = row[1]  # Upper bound of confidence interval\n",
    "            coef = row['coef']  # Coefficient value\n",
    "\n",
    "            ci = [[coef - ci_lower], [ci_upper - coef]]\n",
    "\n",
    "            color = 'tab:red' if row['significant?'] == 'significant' and coef > 0 else \\\n",
    "                    'tab:blue' if row['significant?'] == 'significant' and coef <= 0 else \\\n",
    "                    'tab:gray'\n",
    "\n",
    "            plt.errorbar(x=[coef], y=[idx], xerr=ci, ecolor=color, capsize=3, linestyle='None', \n",
    "                         linewidth=1, marker=\"o\", markersize=5, mfc=color, mec=color)\n",
    "\n",
    "    plt.axvline(x=0, color='grey', linestyle='--')\n",
    "#     plt.yticks(np.arange(len(sorted_coef)), sorted_coef.index)\n",
    "    plt.xlabel('Coefficient Value (CHF)')\n",
    "    plt.ylabel('Coefficient Name')\n",
    "    plt.title('OLS Coefficient Plot - {}'.format(var))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final[data_final.gp == 'LCA & AOS']['Methodes de massage_amount'].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_OLS_and_plot(df, dependent_var, independent_vars):\n",
    "    df['intercept'] = 1  # Add an intercept term\n",
    "    df[dependent_var].fillna(0, inplace=True)\n",
    "    \n",
    "    # Rename columns to remove '_binary'\n",
    "    renamed_independent_vars = [x.replace(\"_binary\", \"\") for x in independent_vars]\n",
    "    df_renamed = df[independent_vars + ['intercept']].rename(columns=dict(zip(independent_vars, renamed_independent_vars)))\n",
    "\n",
    "    # Specify the multiple linear regression model\n",
    "    linear_model = sm.OLS(df[dependent_var], df_renamed)\n",
    "\n",
    "    # Fit the model\n",
    "    result = linear_model.fit()\n",
    "\n",
    "    # Plot the coefficients\n",
    "    plot_OLS_coefficients(result, var.replace(\"_amount\",''))  # Assuming plot_OLS_coefficients function is defined\n",
    "\n",
    "# Variables to consider\n",
    "variables = ['PRESTATIONS_BRUTES_CAM',\n",
    "    'Methodes ayurvediques_amount',\n",
    "    \"Methodes d'art therapie_amount\",\n",
    "    'Methodes de massage_amount',\n",
    "    'Methodes energetiques_amount',\n",
    "    'Methodes energetiques manuelles_amount',\n",
    "    'Methodes hydrotherapeutiques_amount',\n",
    "    'Methodes occidentales_amount',\n",
    "    'Methodes orientales_amount',\n",
    "    'Methodes prescriptives_amount',\n",
    "    'Methodes psychologiques complementaires_amount',\n",
    "    'Methodes reflexes_amount',\n",
    "    'Methodes therapeutiques par le mouvement_amount'\n",
    "]\n",
    "\n",
    "# Assuming 'data_final' is your DataFrame and 'chronic_diseases_binary' contains your independent variables\n",
    "for var in variables:\n",
    "    run_OLS_and_plot(data_final[data_final.gp == 'LCA & AOS'], var, chronic_diseases_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def validate_inputs(df, dependent_var, independent_vars):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input should be a Pandas DataFrame\")\n",
    "    if dependent_var not in df.columns:\n",
    "        raise ValueError(f\"{dependent_var} is not in DataFrame\")\n",
    "    for var in independent_vars:\n",
    "        if var not in df.columns:\n",
    "            raise ValueError(f\"{var} is not in DataFrame\")\n",
    "\n",
    "def fit_logit_model(df, dependent_var, independent_vars):\n",
    "    try:\n",
    "        logit_model = sm.Logit((df[dependent_var] > 0).astype(int), df[independent_vars])\n",
    "        \n",
    "        return logit_model.fit(disp=0)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Logistic Regression failed: {e}\")\n",
    "\n",
    "def fit_ols_model(df, dependent_var, independent_vars):\n",
    "    try:\n",
    "        ols_model = sm.OLS(df[dependent_var], df[independent_vars])\n",
    "        return ols_model.fit(disp=0)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"OLS failed: {e}\")\n",
    "\n",
    "def two_part_model_bootstrap(df, dependent_var, independent_vars, n_iterations=10):\n",
    "    \"\"\"\n",
    "    This function performs bootstrap resampling to create two-part models.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame\n",
    "    dependent_var: str\n",
    "    independent_vars: list\n",
    "    n_iterations: int\n",
    "    \n",
    "    Returns:\n",
    "    bootstrapped_predictions: list\n",
    "    \"\"\"\n",
    "    validate_inputs(df, dependent_var, independent_vars)\n",
    "    \n",
    "    # Immutable copy to avoid altering input\n",
    "    formatted_vars = list(independent_vars) + ['intercept']\n",
    "    \n",
    "    bootstrapped_predictions = []\n",
    "    bootstrapped_logit_coef = []\n",
    "    bootstrapped_ols_coef = []\n",
    "    boostrapped_logit_incurring_costs = []\n",
    "    for i in range(n_iterations):\n",
    "        print('Iteration number {}'.format(i))\n",
    "        df_sample = df.sample(frac=1, replace=True)\n",
    "        df_sample['intercept'] = 1\n",
    "        df_sample.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Logistic regression\n",
    "        logit_result = fit_logit_model(df_sample, dependent_var, formatted_vars)\n",
    "        bootstrapped_logit_coef.append(logit_result.params)\n",
    "\n",
    "        # OLS regression\n",
    "        ols_result = fit_ols_model(df_sample.loc[df_sample[dependent_var] > 0], dependent_var, formatted_vars)\n",
    "        bootstrapped_ols_coef.append(ols_result.params)\n",
    "\n",
    "        # Predictions\n",
    "        final_prediction = logit_result.predict(df_sample[formatted_vars]) * ols_result.predict(df_sample[formatted_vars])\n",
    "        bootstrapped_predictions.append(final_prediction)\n",
    "        boostrapped_logit_incurring_costs.append(logit_result.predict(df_sample[formatted_vars]))\n",
    "        \n",
    "    return {'bootstrapped_predictions': bootstrapped_predictions, 'bootstrapped_ols_coef':bootstrapped_ols_coef,\n",
    "           'bootstrapped_logit_coef':bootstrapped_logit_coef,'boostrapped_logit_incurring_costs':boostrapped_logit_incurring_costs, 'ols_result':ols_result}\n",
    "\n",
    "# Usage\n",
    "dependent_var = 'Methodes de massage_amount'\n",
    "independent_vars = chronic_diseases_binary\n",
    "\n",
    "result = two_part_model_bootstrap(data_final, dependent_var, independent_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "chronic_diseases_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['Migraines_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two-part regression models WITHOUT TRAIN AND TEST SETS\n",
    "X = data_final[['Rheumatologic conditions_binary','Migraines_binary','Psychoses_binary','Epilepsy_binary','Hyperlipidemia_binary']]\n",
    "y = data_final[dependent_var]\n",
    "# Part 1: Logistic regression to determine the probability of incurring health care costs per patient/year\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_2017[predictors + chronic_conditions], data_2017['PRESTATIONS_TOTAL'], test_size=0.2, random_state=42)\n",
    "log_reg.fit(X, y > 0)\n",
    "print(log_reg.intercept_, log_reg.coef_, log_reg.score(X, y>0))\n",
    "\n",
    "prob_incurring_costs = log_reg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costs = X[y > 0]\n",
    "y_costs = y[y > 0]\n",
    "glm_gamma = sm.GLM(endog = y_costs, exog=X_costs, family=sm.families.Gamma(link=sm.families.links.Log()))\n",
    "glm_gamma_results = glm_gamma.fit()\n",
    "predicted_costs = glm_gamma_results.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_costs.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[71322]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_total_costs = prob_incurring_costs * predicted_costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_total_costs.sort_values().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var = 'Methodes de massage_amount'\n",
    "independent_vars = ['Rheumatologic conditions_binary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_gamma = sm.GLM(endog = y_costs, exog=X_costs, family=sm.families.Gamma())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_gamma_results = glm_gamma.fit()\n",
    "predicted_costs = glm_gamma_results.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_total_costs = prob_incurring_costs * predicted_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_total_costs.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "data_final[dependent_var].fillna(0, inplace=True)\n",
    "\n",
    "X = data_final[independent_vars]\n",
    "y = data_final[dependent_var]\n",
    "# Part 1: Logistic regression to determine the probability of incurring health care costs per patient/year\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_2017[predictors + chronic_conditions], data_2017['PRESTATIONS_TOTAL'], test_size=0.2, random_state=42)\n",
    "log_reg.fit(X, y > 0)\n",
    "print(log_reg.intercept_, log_reg.coef_, log_reg.score(X, y>0))\n",
    "\n",
    "prob_incurring_costs = log_reg.predict_proba(X)[:, 1]\n",
    "#     df_cds_year['prob_cost'] = prob_incurring_costs\n",
    "# Part 2: Generalized linear regression with a gamma error distribution and linear link function\n",
    "# to estimate annual health care expenditures for patients incurring costs higher than 0\n",
    "data_final['const'] = 1\n",
    "data_final[independent_vars + ['const']]\n",
    "X_costs = X[y > 0]\n",
    "y_costs = y[y > 0]\n",
    "\n",
    "glm_gamma = sm.GLM(endog = y_costs, exog=X_costs, family=sm.families.Gamma(link=sm.families.links.Identity()))\n",
    "glm_gamma_results = glm_gamma.fit()\n",
    "predicted_costs = glm_gamma_results.predict(X)\n",
    "expected_total_costs = prob_incurring_costs * predicted_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_costs.isna().sum())\n",
    "print(X_costs.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = data_final.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_predictions = pd.DataFrame(result['bootstrapped_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn lists of bootstrapped coefficients into DataFrames\n",
    "bootstrapped_logit_coef_df = pd.DataFrame(result['bootstrapped_logit_coef'])\n",
    "bootstrapped_ols_coef_df = pd.DataFrame(result['bootstrapped_ols_coef'])\n",
    "bootstrapped_predictions = pd.DataFrame(result['bootstrapped_predictions'])\n",
    "\n",
    "# Calculate 95% confidence intervals for logistic regression\n",
    "logit_ci_lower = bootstrapped_logit_coef_df.quantile(0.025)\n",
    "logit_ci_upper = bootstrapped_logit_coef_df.quantile(0.975)\n",
    "\n",
    "# Calculate 95% confidence intervals for OLS regression\n",
    "ols_ci_lower = bootstrapped_ols_coef_df.quantile(0.025)\n",
    "ols_ci_upper = bootstrapped_ols_coef_df.quantile(0.975)\n",
    "\n",
    "# Compute prediction intervals\n",
    "prediction_interval_lower = bootstrapped_predictions.quantile(0.025, axis=1)\n",
    "prediction_interval_upper = bootstrapped_predictions.quantile(0.975, axis=1)\n",
    "\n",
    "# Let's say original_predictions are the predictions from the original model\n",
    "original_predictions = np.mean(bootstrapped_predictions, axis=1)\n",
    "\n",
    "\n",
    "# # Print or store these values for interpretation\n",
    "# print(\"Logit Coefficients 95% CI:\", logit_ci_lower, logit_ci_upper)\n",
    "# print(\"OLS Coefficients 95% CI:\", ols_ci_lower, ols_ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(original_predictions)), original_predictions, color='blue', label='Predictions')\n",
    "plt.fill_between(range(len(original_predictions)), prediction_interval_lower, prediction_interval_upper, color='grey', alpha=0.5, label='95% Prediction Interval')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Prediction Intervals for Bootstrapped Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming bootstrapped_predictions is a DataFrame where each column is a set of predictions from a bootstrap sample\n",
    "bootstrapped_predictions = pd.DataFrame(bootstrapped_predictions)\n",
    "\n",
    "# Compute prediction intervals\n",
    "prediction_interval_lower = bootstrapped_predictions.quantile(0.025, axis=1)\n",
    "prediction_interval_upper = bootstrapped_predictions.quantile(0.975, axis=1)\n",
    "\n",
    "# Let's say original_predictions are the predictions from the original model\n",
    "original_predictions = np.mean(bootstrapped_predictions, axis=1)\n",
    "\n",
    "# Making the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(original_predictions)), original_predictions, color='blue', label='Predictions')\n",
    "plt.fill_between(range(len(original_predictions)), prediction_interval_lower, prediction_interval_upper, color='grey', alpha=0.5, label='95% Prediction Interval')\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Prediction Intervals for Bootstrapped Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_part_model_bootstrap_with_coef(df, dependent_var, independent_vars, n_iterations=100):\n",
    "    bootstrapped_predictions = []\n",
    "    bootstrapped_logit_coef = []\n",
    "    bootstrapped_ols_coef = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Sample with replacement from original data\n",
    "        df_sample = resample(df, replace=True)\n",
    "        \n",
    "        # Prepare independent variables\n",
    "        df_sample['intercept'] = 1\n",
    "        renamed_independent_vars = [x.replace(\"_binary\", \"\") for x in independent_vars]\n",
    "        df_renamed = df_sample[independent_vars + ['intercept']].rename(columns=dict(zip(independent_vars, renamed_independent_vars)))\n",
    "        \n",
    "        # Part 1: Logistic regression for zero vs. non-zero outcome\n",
    "        logit_model = sm.Logit((df_sample[dependent_var] > 0).astype(int), df_renamed)\n",
    "        logit_result = logit_model.fit(disp=0)\n",
    "        bootstrapped_logit_coef.append(logit_result.params)\n",
    "        \n",
    "        # Part 2: OLS for non-zero outcomes\n",
    "        ols_model = sm.OLS(df_sample.loc[df_sample[dependent_var] > 0, dependent_var], df_renamed.loc[df_sample[dependent_var] > 0])\n",
    "        ols_result = ols_model.fit(disp=0)\n",
    "        bootstrapped_ols_coef.append(ols_result.params)\n",
    "        \n",
    "        # Predictions\n",
    "        prob_non_zero = logit_result.predict(df_renamed)\n",
    "        ols_prediction = ols_result.predict(df_renamed)\n",
    "        \n",
    "        # Combined prediction\n",
    "        final_prediction = prob_non_zero * ols_prediction\n",
    "        bootstrapped_predictions.append(final_prediction)\n",
    "        \n",
    "    return bootstrapped_predictions, bootstrapped_logit_coef, bootstrapped_ols_coef\n",
    "\n",
    "# Example usage\n",
    "dependent_var = 'PRESTATIONS_BRUTES_CAM'\n",
    "independent_vars = chronic_diseases_binary\n",
    "bootstrapped_predictions, bootstrapped_logit_coef, bootstrapped_ols_coef = two_part_model_bootstrap_with_coef(data_final, dependent_var, independent_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder  = Path('../Data/')\n",
    "\n",
    "df_amount_by_lca_discipline = pd.read_parquet(data_folder/'processed'/'Intermediate datasets'/'df_amount_by_lca_discipline.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Association between disease apparition and complementary medicine usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_uuid = data_final.head(5000).uuid.unique()\n",
    "df_sample = data_final[data_final.uuid.isin(df_sample_uuid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['Pain_diff'] = df_sample.sort_values(['uuid','NOANNEE']).groupby('uuid')['Pain_binary'].diff(1)\n",
    "df_sample['CDS_diff'] = df_sample.sort_values(['uuid','NOANNEE']).groupby('uuid')['cds'].diff(1)\n",
    "\n",
    "df_sample['Psy_diff'] = df_sample.sort_values(['uuid','NOANNEE']).groupby('uuid')['Psychological disorders_binary'].diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[['PRESTATIONS_BRUTES_LCA','PRESTATIONS_BRUTES_AOS','CDS_diff','cds']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.groupby('Psy_diff')['PRESTATIONS_BRUTES_LCA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_change(group, var):\n",
    "    diff = group[var].diff()\n",
    "    return diff\n",
    "var = 'Pain_binary'\n",
    "# Group by PatientID and apply the custom function\n",
    "changes = df_sample.groupby('uuid').apply(lambda x: check_change(x, var)).reset_index()\n",
    "\n",
    "# Patients who changed from 0 to 1\n",
    "increased_status = changes.loc[changes[var] == 1, 'uuid'].unique()\n",
    "\n",
    "# Patients who changed from 1 to 0\n",
    "decreased_status = changes.loc[changes[var] == -1, 'uuid'].unique()\n",
    "\n",
    "# Filter the DataFrame for each case\n",
    "df_increased = df_sample[df_sample['uuid'].isin(increased_status)]\n",
    "df_decreased = df_sample[df_sample['uuid'].isin(decreased_status)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['mammo_last_invite'] = df_final[['numerodossier','mammo']].groupby('numerodossier').diff()['mammo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.groupby('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### 3. Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "# Choose hyperparameter domain to search over\n",
    "space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(2, 30, 2, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.3, 1.01, 0.1),\n",
    "        'min_child_weight':hp.choice('min_child_weight', np.arange(1, 30, 1, dtype=int)),\n",
    "        'subsample':        hp.quniform('subsample', 0.3, 1.01, 0.1),\n",
    "        'learning_rate':    hp.choice('learning_rate',    np.arange(0.1, 1.01, 0.1)),\n",
    "        'gamma': hp.quniform('gamma', 0.1, 5, 0.2),\n",
    "    \n",
    "        'objective':'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "    }\n",
    "\n",
    "def score(params, n_folds=5):\n",
    "    _X_coords_sample = X_coords.sample(frac = 0.05, random_state=333).sort_index()\n",
    "    _y_sample = y[y.index.isin(_X_coords_sample.index)]\n",
    "    #Cross-validation\n",
    "    d_train = xgboost.DMatrix(_X_coords_sample,_y_sample)\n",
    "    \n",
    "    cv_results = xgboost.cv(params, d_train, nfold = n_folds, num_boost_round=500,\n",
    "                        early_stopping_rounds = 10, metrics = 'rmse', seed = 0)\n",
    "    \n",
    "    loss = min(cv_results['test-rmse-mean'])\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def optimize(trials, space, n_evals):\n",
    "    \n",
    "    best = fmin(score, space, algo=tpe.suggest, max_evals=n_evals,trials=trials,\n",
    "                rstate=np.random.default_rng(333))#Add seed to fmin function\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretable_ml_shap_viz_gpt(df, model, X, model_folder, include_spatial):\n",
    "    model_directory = res_folder / model_folder\n",
    "    \n",
    "    explainer_shap = shap.TreeExplainer(model)\n",
    "    shap_values = explainer_shap(X)\n",
    "    shap_interaction_values = explainer_shap.shap_interaction_values(X)\n",
    "    \n",
    "    # Shapley summary\n",
    "    shap.summary_plot(shap_values, feature_names=X.columns, show=False)\n",
    "    plt.savefig(model_directory / 'shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Shapley interaction values\n",
    "    shap.summary_plot(shap_interaction_values, X, max_display=16,\n",
    "                      feature_names=X.columns, show=False, plot_type=\"compact_dot\")\n",
    "    plt.savefig(model_directory / 'shap_summary_interaction_values.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Shapley dependence plots\n",
    "    dependence_dir = model_directory / 'Dependence plots'\n",
    "    if not os.path.exists(dependence_dir):\n",
    "        os.makedirs(dependence_dir)\n",
    "    \n",
    "    for name in X.columns:\n",
    "        shap.dependence_plot(name, shap_values.values, X, display_features=X, show=False)\n",
    "        plt.savefig(dependence_dir / f'shap_dependence_{name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Calculate the number of rows and columns for the subplots\n",
    "    n_columns = len(X.columns)\n",
    "    n_rows = int(np.ceil(n_columns / 3))\n",
    "\n",
    "    # Maps of SHAP variables\n",
    "    if include_spatial:\n",
    "        fig, ax = plt.subplots(n_rows, 3, figsize=(15, 15 * n_rows / 3))\n",
    "        ax = ax.ravel()\n",
    "\n",
    "        for j in range(n_columns):\n",
    "            df.plot(ax=ax[j], column=shap_values.values[:, j], legend=True, markersize = 1,\n",
    "                    vmin=-shap_values.values[:, j].max(), vmax=shap_values.values[:, j].max(), cmap=shap.plots.colors.red_white_blue, legend_kwds={'shrink':0.5})\n",
    "            ax[j].set_title(\"SHAP for\\n\" + str(X.columns[j]), fontsize=10)\n",
    "            ax[j].set_axis_off()\n",
    "\n",
    "        # Hide remaining unused subplots\n",
    "        for j in range(n_columns, n_rows * 3):\n",
    "            ax[j].set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_directory / 'maps_shap_variables.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Map of location effects\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "        df.plot(ax=ax, column=shap_values.values[:, -1] + shap_values.values[:, -2],\n",
    "                legend=True, vmin=-(shap_values.values[:, -1] + shap_values.values[:, -2]).max(), vmax=(shap_values.values[:, -1] + shap_values.values[:, -2]).max(), figsize=(15, 8), markersize = 1,\n",
    "                cmap=shap.plots.colors.red_white_blue)\n",
    "        plt.title(\"Location Effect on Cluster persistence\\n(SHAP values of geographic coordinates)\\n\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(model_directory/ 'maps_shap_location.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Variable importance\n",
    "    xgboost.plot_importance(model)\n",
    "    plt.title(\"xgboost.plot_importance(model)\")\n",
    "    plt.savefig(model_directory / 'importance_plot_default.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    xgboost.plot_importance(model, importance_type=\"cover\")\n",
    "    plt.title('xgboost.plot_importance(model, importance_type=\"cover\")')\n",
    "    plt.savefig(model_directory / 'importance_plot_cover.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Force plots\n",
    "    shap.force_plot(explainer_shap.expected_value, shap_values.values[1, :], X.iloc[1, :], show=False, matplotlib=True)\n",
    "    plt.savefig(model_directory / 'force_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_xgboost(gwr_df, X_eq, y_col, optimize, space, n_evals, file_prefix, include_spatial=True):\n",
    "    model_directory = res_folder / file_prefix\n",
    "    if not os.path.exists(model_directory):\n",
    "        os.makedirs(model_directory)\n",
    "    if include_spatial:\n",
    "        X_coords = gwr_df[X_eq + ['E', 'N']]\n",
    "    else:\n",
    "        X_coords = gwr_df[X_eq]\n",
    "    \n",
    "    y = gwr_df[y_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_coords, y, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=0)\n",
    "     ### Hyperparameter tuning\n",
    "    if not os.path.exists(model_directory/'best_params.pkl'):\n",
    "\n",
    "        trials = Trials()\n",
    "        best_params = optimize(trials, space, n_evals=n_evals)\n",
    "\n",
    "        # Return the best parameters\n",
    "        best_params = space_eval(space, best_params)\n",
    "\n",
    "        with open(model_directory / 'trials.pkl', 'wb') as f:\n",
    "            pickle.dump(trials, f)\n",
    "\n",
    "        with open(model_directory / 'best_params.pkl', 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n",
    "    else:\n",
    "        best_params = np.load(model_directory / 'best_params.pkl', allow_pickle=True)\n",
    "\n",
    "        with open(model_directory/ 'trials.pkl', 'rb') as f:\n",
    "            trials = pickle.load(f)\n",
    "\n",
    "\n",
    "    ### Train model\n",
    "    d_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "    d_test = xgboost.DMatrix(X_test, label=y_test)\n",
    "    d_all = xgboost.DMatrix(X_coords, label=y)\n",
    "\n",
    "    final_model = xgboost.train(best_params,  d_train, num_boost_round=500, evals=[(d_train, \"train\"), (d_test, \"test\")], \n",
    "                                verbose_eval=False, early_stopping_rounds=10)\n",
    "\n",
    "    y_pred_t = final_model.predict(d_test)\n",
    "\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_t)))\n",
    "    print(\"R2:\", r2_score(y_test, y_pred_t))\n",
    "\n",
    "    data_test = xgboost.DMatrix(X_test, y_test)\n",
    "    data = xgboost.DMatrix(X_train, y_train)\n",
    "    data_all = xgboost.DMatrix(X_coords)\n",
    "    \n",
    "    ### Fit model or retrieve it\n",
    "    if not os.path.exists(model_directory/'model.pkl'):\n",
    "        final_model = xgboost.train(best_params, data, num_boost_round=5000, verbose_eval=False,\n",
    "                                evals=[(data_test, \"Test\")], early_stopping_rounds=10)\n",
    "        with open(model_directory/'model.pkl', 'wb') as f:\n",
    "            pickle.dump(final_model, f)\n",
    "    else:\n",
    "        final_model = np.load(model_directory / 'model.pkl', allow_pickle=True)\n",
    "\n",
    "    y_pred = final_model.predict(data_all)\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y, y_pred)))\n",
    "    print('R2: ', r2_score(y, y_pred))\n",
    "    \n",
    "    interpretable_ml_shap_viz_gpt(gwr_df, final_model, X_coords, file_prefix, include_spatial)\n",
    "\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_2017['PRESTATIONS_TOTAL']\n",
    "X_eq0 = ['NBAGE','DEDUCTIBLE_above_500','SEX_F','CANTON_NAME_Genève','CANTON_NAME_Vaud','CANTON_NAME_Valais','ssep2']\n",
    "X_coords = data_2017[X_eq0 + ['E','N']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_2017 = gpd.GeoDataFrame(data_2017, crs = 2056, geometry = data_2017['geometry'] )\n",
    "# data_2018 = gpd.GeoDataFrame(data_2018, crs = 2056, geometry = data_2018['geometry'] )\n",
    "# data_2019 = gpd.GeoDataFrame(data_2019, crs = 2056, geometry = data_2019['geometry'] )\n",
    "# data_2020 = gpd.GeoDataFrame(data_2020, crs = 2056, geometry = data_2020['geometry'] )\n",
    "# data_2021 = gpd.GeoDataFrame(data_2021, crs = 2056, geometry = data_2021['geometry'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(data_2017, X_eq0, 'PRESTATIONS_TOTAL', optimize, space, n_evals=3000, file_prefix='Model 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_shap = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer_shap(X_coords)\n",
    "shap_interaction_values = explainer_shap.shap_interaction_values(X_coords)\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "n_columns = len(X_coords.columns)\n",
    "n_rows = int(np.ceil(n_columns / 3))\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, 3, figsize=(15, 15 * n_rows / 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for j in range(n_columns):\n",
    "    data_2017.plot(ax=ax[j], column=shap_values.values[:, j], legend=True, markersize = 1,\n",
    "            vmin=-shap_values.values[:, j].max(), vmax=shap_values.values[:, j].max(), cmap=shap.plots.colors.red_white_blue, legend_kwds={'shrink':0.5})\n",
    "    ax[j].set_title(\"SHAP for\\n\" + str(X_coords.columns[j]), fontsize=10)\n",
    "    ax[j].set_axis_off()\n",
    "\n",
    "# Hide remaining unused subplots\n",
    "for j in range(n_columns, n_rows * 3):\n",
    "    ax[j].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_directory/'trials.pkl', 'rb') as f:\n",
    "    trials = pickle.load(f)\n",
    "trials = pd.DataFrame.from_dict(trials.results)\n",
    "trials = trials.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['rolling_loss'] = trials['loss'].rolling(100).mean()\n",
    "trials.plot(x = 'index', y = 'rolling_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_coords = data_2017[X_eq0 + ['E', 'N']]\n",
    "y = data_2017['treatment']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_coords, y, \n",
    "#                                                     test_size=0.2, \n",
    "#                                                     random_state=0)\n",
    "# ### Hyperparameter tuning\n",
    "# trials = Trials()\n",
    "# best_params = optimize(trials, space, n_evals=500)\n",
    "\n",
    "# # Return the best parameters\n",
    "# best_params = space_eval(space, best_params)\n",
    "\n",
    "# with open(file_prefix + '_trials.pkl', 'wb') as f:\n",
    "#     pickle.dump(trials, f)\n",
    "\n",
    "# with open(file_prefix + '_best_params.pkl', 'wb') as f:\n",
    "#     pickle.dump(best_params, f)\n",
    "\n",
    "# best_params = np.load(file_prefix + '_best_params.pkl', allow_pickle=True)\n",
    "\n",
    "# with open(file_prefix + '_trials.pkl', 'rb') as f:\n",
    "#     trials = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
